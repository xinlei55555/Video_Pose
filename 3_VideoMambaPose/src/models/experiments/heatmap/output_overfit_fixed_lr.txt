[2024-06-09 00:54:19,621] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Read successful, here are the characteristics of your model: 
{'model_name': 'VideoMambaPose', 'project_dir': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/latent_space_regression_with_linear', 'model_type': 'latent_space_linear_regression', 'full_debug': False, 'show_gradients': False, 'show_predictions': False, 'embed_channels': 192, 'num_mamba_blocks': 12, 'num_deconv': 2, '2d_deconv': True, 'deconv_channels': 192, 'num_conv': 2, 'conv_channels': 256, 'joint_regressor': True, 'hidden_channels': 512, 'output_dimensions': 2, 'dropout': False, 'dropout_percent': 0.25, 'num_hidden_layers': 3, 'epoch_number': 300, 'batch_size': 4, 'learning_rate': 0.001, 'scheduler': True, 'start_epoch': 1, 'num_cpus': 1, 'num_gpus': 1, 'parallelize': False, 'checkpoint_directory': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/heatmap/checkpoint', 'checkpoint_name': 'tanh_overfit_model', 'follow_up': False, 'previous_training_epoch': 1, 'previous_checkpoint': '', 'dataset_name': 'JHMDB', 'data_path': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/data/JHMDB', 'annotations_path': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/data/JHMDB_old/annotations', 'use_videos': False, 'skip': ['wave'], 'num_frames': 16, 'channels': 3, 'image_height': 240, 'image_width': 320, 'image_tensor_height': 192, 'image_tensor_width': 256, 'patch_number': 192, 'patch_size': 16, 'jump': 1, 'joint_number': 15, 'real_job': False, 'normalized': True, 'default': False, 'min_norm': -1}
length of actions 1
The length of actions, train and test are 1 ,  21 ,  0
length of actions 1
The length of actions, train and test are 1 ,  0 ,  2
num_workers is: 7, for 8 cores
Use checkpoint: False
Checkpoint number: 0
Model loaded successfully as follows:  HeatMapVideoMambaPose(
  (mamba): VisionMamba(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 192, kernel_size=(1, 16, 16), stride=(1, 16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (head_drop): Identity()
    (head): Linear(in_features=192, out_features=1000, bias=True)
    (drop_path): DropPath()
    (layers): ModuleList(
      (0-1): 2 x Block(
        (mixer): Mamba(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (act): SiLU()
          (x_proj): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj): Linear(in_features=12, out_features=384, bias=True)
          (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
        (norm): RMSNorm()
        (drop_path): Identity()
      )
      (2-11): 10 x Block(
        (mixer): Mamba(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (act): SiLU()
          (x_proj): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj): Linear(in_features=12, out_features=384, bias=True)
          (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
        (norm): RMSNorm()
        (drop_path): DropPath()
      )
    )
    (norm_f): RMSNorm()
  )
  (deconv): Deconv(
    (conv_layers): Sequential(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))
    )
    (deconv_layers): Sequential(
      (0): ConvTranspose2d(192, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (joints): JointOutput(
    (regressor): Sequential(
      (0): Linear(in_features=3072, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=2, bias=True)
      (3): Tanh()
    )
  )
)
The model has started training, with the following characteristics:
Epoch 1 started ======>
Memory before (in MB) 29.263872
The number of batches in the train_set is 124
The number of batches in the test_set is 12
train batch for epoch #  1 ==============>
The shape of the outputs is  torch.Size([4, 15, 2])
The shape of the labels are  torch.Size([4, 15, 2])
Memory after train_batch (in MB) 172.51328
test batch for epoch #  1 ======================>
Memory after test_batch (in MB) 190.864896
Scheduler is currently registering the learning rate.
Epoch 1, Pointwise Training loss 0.5023994597215806, Pointwise Validation loss 0.3152082984646161
Full training loss: 62.297533005476, Full test loss: 3.7824995815753937
The current learning rate is: 0.001
Best model saved at /home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/heatmap/checkpoint/tanh_overfit_model/heatmap_3.7825.pt
Model parameters are of the following size 231
Epoch 2 started ======>
train batch for epoch #  2 ==============>
test batch for epoch #  2 ======================>
Scheduler is currently registering the learning rate.
Epoch 2, Pointwise Training loss 0.33640913136543765, Pointwise Validation loss 0.31702033927043277
Full training loss: 41.71473228931427, Full test loss: 3.8042440712451935
The current learning rate is: 0.001
Epoch 3 started ======>
train batch for epoch #  3 ==============>
test batch for epoch #  3 ======================>
Scheduler is currently registering the learning rate.
Epoch 3, Pointwise Training loss 0.3333045305503953, Pointwise Validation loss 0.28930123647054035
Full training loss: 41.329761788249016, Full test loss: 3.4716148376464844
The current learning rate is: 0.001
Best model saved at /home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/heatmap/checkpoint/tanh_overfit_model/heatmap_3.4716.pt
Model parameters are of the following size 231
Epoch 4 started ======>
train batch for epoch #  4 ==============>
test batch for epoch #  4 ======================>
Scheduler is currently registering the learning rate.
Epoch 4, Pointwise Training loss 0.33234407824854695, Pointwise Validation loss 0.288962888220946
Full training loss: 41.210665702819824, Full test loss: 3.467554658651352
The current learning rate is: 0.001
Best model saved at /home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/heatmap/checkpoint/tanh_overfit_model/heatmap_3.4676.pt
Model parameters are of the following size 231
Epoch 5 started ======>
train batch for epoch #  5 ==============>
test batch for epoch #  5 ======================>
Scheduler is currently registering the learning rate.
Epoch 5, Pointwise Training loss 0.3313924328934762, Pointwise Validation loss 0.29714016367991763
Full training loss: 41.092661678791046, Full test loss: 3.565681964159012
The current learning rate is: 0.001
Epoch 6 started ======>
train batch for epoch #  6 ==============>
test batch for epoch #  6 ======================>
Scheduler is currently registering the learning rate.
Epoch 6, Pointwise Training loss 0.3329315590521982, Pointwise Validation loss 0.29576802998781204
Full training loss: 41.28351332247257, Full test loss: 3.5492163598537445
The current learning rate is: 0.001
Epoch 7 started ======>
train batch for epoch #  7 ==============>
test batch for epoch #  7 ======================>
Scheduler is currently registering the learning rate.
Epoch 7, Pointwise Training loss 0.3309410037773271, Pointwise Validation loss 0.28945408513148624
Full training loss: 41.03668446838856, Full test loss: 3.473449021577835
The current learning rate is: 0.001
Epoch 8 started ======>
train batch for epoch #  8 ==============>
test batch for epoch #  8 ======================>
Scheduler is currently registering the learning rate.
Epoch 8, Pointwise Training loss 0.33096184040750226, Pointwise Validation loss 0.296528585255146
Full training loss: 41.03926821053028, Full test loss: 3.5583430230617523
The current learning rate is: 0.001
Epoch 9 started ======>
train batch for epoch #  9 ==============>
test batch for epoch #  9 ======================>
Scheduler is currently registering the learning rate.
Epoch 9, Pointwise Training loss 0.3308370128994988, Pointwise Validation loss 0.2989581376314163
Full training loss: 41.02378959953785, Full test loss: 3.587497651576996
The current learning rate is: 0.001
Epoch 10 started ======>
train batch for epoch #  10 ==============>
test batch for epoch #  10 ======================>
Scheduler is currently registering the learning rate.
Epoch 10, Pointwise Training loss 0.33070844892532597, Pointwise Validation loss 0.29197704295317334
Full training loss: 41.00784766674042, Full test loss: 3.50372451543808
The current learning rate is: 0.001
Epoch 11 started ======>
train batch for epoch #  11 ==============>
test batch for epoch #  11 ======================>
Scheduler is currently registering the learning rate.
Epoch 11, Pointwise Training loss 0.3315840027024669, Pointwise Validation loss 0.29874694844086963
Full training loss: 41.116416335105896, Full test loss: 3.584963381290436
The current learning rate is: 0.001
Epoch 12 started ======>
train batch for epoch #  12 ==============>
test batch for epoch #  12 ======================>
Scheduler is currently registering the learning rate.
Epoch 12, Pointwise Training loss 0.33139056760457253, Pointwise Validation loss 0.2988025024533272
Full training loss: 41.092430382966995, Full test loss: 3.585630029439926
The current learning rate is: 0.001
Epoch 13 started ======>
train batch for epoch #  13 ==============>
test batch for epoch #  13 ======================>
Scheduler is currently registering the learning rate.
Epoch 13, Pointwise Training loss 0.32976727367889497, Pointwise Validation loss 0.30022216588258743
Full training loss: 40.891141936182976, Full test loss: 3.602665990591049
The current learning rate is: 0.001
Epoch 14 started ======>
train batch for epoch #  14 ==============>
test batch for epoch #  14 ======================>
Scheduler is currently registering the learning rate.
Epoch 14, Pointwise Training loss 0.3307246928734164, Pointwise Validation loss 0.29819946239391965
Full training loss: 41.009861916303635, Full test loss: 3.5783935487270355
The current learning rate is: 0.001
Epoch 15 started ======>
train batch for epoch #  15 ==============>
test batch for epoch #  15 ======================>
Scheduler is currently registering the learning rate.
Epoch 15, Pointwise Training loss 0.3300893983292964, Pointwise Validation loss 0.2937057266632716
Full training loss: 40.931085392832756, Full test loss: 3.524468719959259
The current learning rate is: 0.0005
Epoch 16 started ======>
train batch for epoch #  16 ==============>
test batch for epoch #  16 ======================>
Scheduler is currently registering the learning rate.
Epoch 16, Pointwise Training loss 0.32984729519774836, Pointwise Validation loss 0.3033813163638115
Full training loss: 40.9010646045208, Full test loss: 3.640575796365738
The current learning rate is: 0.0005
Epoch 17 started ======>
train batch for epoch #  17 ==============>
test batch for epoch #  17 ======================>
Scheduler is currently registering the learning rate.
Epoch 17, Pointwise Training loss 0.33018046136825313, Pointwise Validation loss 0.29145005345344543
Full training loss: 40.94237720966339, Full test loss: 3.497400641441345
The current learning rate is: 0.0005
Epoch 18 started ======>
train batch for epoch #  18 ==============>
test batch for epoch #  18 ======================>
Scheduler is currently registering the learning rate.
Epoch 18, Pointwise Training loss 0.32954373631265854, Pointwise Validation loss 0.2876415128509204
Full training loss: 40.86342330276966, Full test loss: 3.4516981542110443
The current learning rate is: 0.0005
Best model saved at /home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/heatmap/checkpoint/tanh_overfit_model/heatmap_3.4517.pt
Model parameters are of the following size 231
Epoch 19 started ======>
train batch for epoch #  19 ==============>
test batch for epoch #  19 ======================>
Scheduler is currently registering the learning rate.
Epoch 19, Pointwise Training loss 0.3295319716055547, Pointwise Validation loss 0.2873849992950757
Full training loss: 40.86196447908878, Full test loss: 3.448619991540909
The current learning rate is: 0.0005
Best model saved at /home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/heatmap/checkpoint/tanh_overfit_model/heatmap_3.4486.pt
Model parameters are of the following size 231
Epoch 20 started ======>
train batch for epoch #  20 ==============>
test batch for epoch #  20 ======================>
Scheduler is currently registering the learning rate.
Epoch 20, Pointwise Training loss 0.3290652814171007, Pointwise Validation loss 0.2994253362218539
Full training loss: 40.80409489572048, Full test loss: 3.5931040346622467
The current learning rate is: 0.0005
Epoch 21 started ======>
train batch for epoch #  21 ==============>
test batch for epoch #  21 ======================>
Scheduler is currently registering the learning rate.
Epoch 21, Pointwise Training loss 0.32955366132720826, Pointwise Validation loss 0.29466922332843143
Full training loss: 40.86465400457382, Full test loss: 3.5360306799411774
The current learning rate is: 0.0005
Epoch 22 started ======>
train batch for epoch #  22 ==============>
test batch for epoch #  22 ======================>
Scheduler is currently registering the learning rate.
Epoch 22, Pointwise Training loss 0.3297035416528102, Pointwise Validation loss 0.2936593641837438
Full training loss: 40.88323916494846, Full test loss: 3.5239123702049255
The current learning rate is: 0.0005
Epoch 23 started ======>
train batch for epoch #  23 ==============>
test batch for epoch #  23 ======================>
Scheduler is currently registering the learning rate.
Epoch 23, Pointwise Training loss 0.33019668260408985, Pointwise Validation loss 0.31025223682324093
Full training loss: 40.94438864290714, Full test loss: 3.723026841878891
The current learning rate is: 0.0005
Epoch 24 started ======>
train batch for epoch #  24 ==============>
test batch for epoch #  24 ======================>
Scheduler is currently registering the learning rate.
Epoch 24, Pointwise Training loss 0.3303947133883353, Pointwise Validation loss 0.3036242946982384
Full training loss: 40.96894446015358, Full test loss: 3.6434915363788605
The current learning rate is: 0.0005
Epoch 25 started ======>
train batch for epoch #  25 ==============>
test batch for epoch #  25 ======================>
Scheduler is currently registering the learning rate.
Epoch 25, Pointwise Training loss 0.3299918361008167, Pointwise Validation loss 0.30136122057835263
Full training loss: 40.918987676501274, Full test loss: 3.6163346469402313
The current learning rate is: 0.0005
Epoch 26 started ======>
train batch for epoch #  26 ==============>
test batch for epoch #  26 ======================>
Scheduler is currently registering the learning rate.
Epoch 26, Pointwise Training loss 0.3293924855609094, Pointwise Validation loss 0.2921731447180112
Full training loss: 40.844668209552765, Full test loss: 3.5060777366161346
The current learning rate is: 0.0005
Epoch 27 started ======>
train batch for epoch #  27 ==============>
test batch for epoch #  27 ======================>
Scheduler is currently registering the learning rate.
Epoch 27, Pointwise Training loss 0.33002232808259224, Pointwise Validation loss 0.30152563750743866
Full training loss: 40.92276868224144, Full test loss: 3.618307650089264
The current learning rate is: 0.0005
Epoch 28 started ======>
train batch for epoch #  28 ==============>
test batch for epoch #  28 ======================>
Scheduler is currently registering the learning rate.
Epoch 28, Pointwise Training loss 0.3296653321914135, Pointwise Validation loss 0.29316239804029465
Full training loss: 40.87850119173527, Full test loss: 3.5179487764835358
The current learning rate is: 0.0005
Epoch 29 started ======>
train batch for epoch #  29 ==============>
test batch for epoch #  29 ======================>
Scheduler is currently registering the learning rate.
Epoch 29, Pointwise Training loss 0.3298999750085415, Pointwise Validation loss 0.2969220355153084
Full training loss: 40.90759690105915, Full test loss: 3.5630644261837006
The current learning rate is: 0.0005
Epoch 30 started ======>
train batch for epoch #  30 ==============>
test batch for epoch #  30 ======================>
Scheduler is currently registering the learning rate.
Epoch 30, Pointwise Training loss 0.32926974337427845, Pointwise Validation loss 0.29846322784821194
Full training loss: 40.82944817841053, Full test loss: 3.581558734178543
The current learning rate is: 0.00025
Epoch 31 started ======>
train batch for epoch #  31 ==============>
test batch for epoch #  31 ======================>
Scheduler is currently registering the learning rate.
Epoch 31, Pointwise Training loss 0.3300554400249835, Pointwise Validation loss 0.2927370071411133
Full training loss: 40.926874563097954, Full test loss: 3.5128440856933594
The current learning rate is: 0.00025
Epoch 32 started ======>
train batch for epoch #  32 ==============>
test batch for epoch #  32 ======================>
Scheduler is currently registering the learning rate.
Epoch 32, Pointwise Training loss 0.3292734288880902, Pointwise Validation loss 0.2918725584944089
Full training loss: 40.829905182123184, Full test loss: 3.502470701932907
The current learning rate is: 0.00025
Epoch 33 started ======>
train batch for epoch #  33 ==============>
test batch for epoch #  33 ======================>
Scheduler is currently registering the learning rate.
Epoch 33, Pointwise Training loss 0.3296099583948812, Pointwise Validation loss 0.2944784313440323
Full training loss: 40.87163484096527, Full test loss: 3.5337411761283875
The current learning rate is: 0.00025
Epoch 34 started ======>
train batch for epoch #  34 ==============>
test batch for epoch #  34 ======================>
Scheduler is currently registering the learning rate.
Epoch 34, Pointwise Training loss 0.32917031405433533, Pointwise Validation loss 0.29828207691510517
Full training loss: 40.81711894273758, Full test loss: 3.579384922981262
The current learning rate is: 0.00025
Epoch 35 started ======>
train batch for epoch #  35 ==============>
test batch for epoch #  35 ======================>
Scheduler is currently registering the learning rate.
Epoch 35, Pointwise Training loss 0.3300460822159244, Pointwise Validation loss 0.29438071697950363
Full training loss: 40.92571419477463, Full test loss: 3.5325686037540436
The current learning rate is: 0.00025
Epoch 36 started ======>
train batch for epoch #  36 ==============>
test batch for epoch #  36 ======================>
Scheduler is currently registering the learning rate.
Epoch 36, Pointwise Training loss 0.3289211648366144, Pointwise Validation loss 0.29689116527636844
Full training loss: 40.78622443974018, Full test loss: 3.5626939833164215
The current learning rate is: 0.00025
Epoch 37 started ======>
train batch for epoch #  37 ==============>
test batch for epoch #  37 ======================>
Scheduler is currently registering the learning rate.
Epoch 37, Pointwise Training loss 0.32943362310048074, Pointwise Validation loss 0.292445607483387
Full training loss: 40.84976926445961, Full test loss: 3.509347289800644
The current learning rate is: 0.00025
Epoch 38 started ======>
train batch for epoch #  38 ==============>
test batch for epoch #  38 ======================>
Scheduler is currently registering the learning rate.
Epoch 38, Pointwise Training loss 0.3294494177304929, Pointwise Validation loss 0.29214483747879666
Full training loss: 40.85172779858112, Full test loss: 3.5057380497455597
The current learning rate is: 0.00025
Epoch 39 started ======>
train batch for epoch #  39 ==============>
test batch for epoch #  39 ======================>
Scheduler is currently registering the learning rate.
Epoch 39, Pointwise Training loss 0.32921820878982544, Pointwise Validation loss 0.2970273445049922
Full training loss: 40.823057889938354, Full test loss: 3.564328134059906
The current learning rate is: 0.00025
Epoch 40 started ======>
train batch for epoch #  40 ==============>
test batch for epoch #  40 ======================>
Scheduler is currently registering the learning rate.
Epoch 40, Pointwise Training loss 0.32927179240411325, Pointwise Validation loss 0.29334507137537
Full training loss: 40.829702258110046, Full test loss: 3.5201408565044403
The current learning rate is: 0.00025
Epoch 41 started ======>
train batch for epoch #  41 ==============>
test batch for epoch #  41 ======================>
Scheduler is currently registering the learning rate.
Epoch 41, Pointwise Training loss 0.3304562337936894, Pointwise Validation loss 0.32878411064545315
Full training loss: 40.97657299041748, Full test loss: 3.9454093277454376
The current learning rate is: 0.000125
Epoch 42 started ======>
train batch for epoch #  42 ==============>
test batch for epoch #  42 ======================>
Scheduler is currently registering the learning rate.
Epoch 42, Pointwise Training loss 0.32973873074496945, Pointwise Validation loss 0.2985319718718529
Full training loss: 40.88760261237621, Full test loss: 3.5823836624622345
The current learning rate is: 0.000125
Epoch 43 started ======>
train batch for epoch #  43 ==============>
test batch for epoch #  43 ======================>
Scheduler is currently registering the learning rate.
Epoch 43, Pointwise Training loss 0.3291624578977785, Pointwise Validation loss 0.29449017345905304
Full training loss: 40.81614477932453, Full test loss: 3.5338820815086365
The current learning rate is: 0.000125
Epoch 44 started ======>
train batch for epoch #  44 ==============>
test batch for epoch #  44 ======================>
Scheduler is currently registering the learning rate.
Epoch 44, Pointwise Training loss 0.3295326324239854, Pointwise Validation loss 0.2968475818634033
Full training loss: 40.86204642057419, Full test loss: 3.56217098236084
The current learning rate is: 0.000125
Epoch 45 started ======>
train batch for epoch #  45 ==============>
test batch for epoch #  45 ======================>
Scheduler is currently registering the learning rate.
Epoch 45, Pointwise Training loss 0.32944148454454636, Pointwise Validation loss 0.29420455545186996
Full training loss: 40.85074408352375, Full test loss: 3.5304546654224396
The current learning rate is: 0.000125
Epoch 46 started ======>
train batch for epoch #  46 ==============>
test batch for epoch #  46 ======================>
Scheduler is currently registering the learning rate.
Epoch 46, Pointwise Training loss 0.32914180404716925, Pointwise Validation loss 0.29501668860514957
Full training loss: 40.813583701848984, Full test loss: 3.540200263261795
The current learning rate is: 0.000125
Epoch 47 started ======>
train batch for epoch #  47 ==============>
test batch for epoch #  47 ======================>
Scheduler is currently registering the learning rate.
Epoch 47, Pointwise Training loss 0.32936254840704704, Pointwise Validation loss 0.29569800446430844
Full training loss: 40.84095600247383, Full test loss: 3.548376053571701
The current learning rate is: 0.000125
Epoch 48 started ======>
train batch for epoch #  48 ==============>
test batch for epoch #  48 ======================>
Scheduler is currently registering the learning rate.
Epoch 48, Pointwise Training loss 0.32938515034414106, Pointwise Validation loss 0.2957954580585162
Full training loss: 40.84375864267349, Full test loss: 3.549545496702194
The current learning rate is: 0.000125
Epoch 49 started ======>
train batch for epoch #  49 ==============>
test batch for epoch #  49 ======================>
Scheduler is currently registering the learning rate.
Epoch 49, Pointwise Training loss 0.3291110659558927, Pointwise Validation loss 0.29416782160600025
Full training loss: 40.80977217853069, Full test loss: 3.530013859272003
The current learning rate is: 0.000125
Epoch 50 started ======>
train batch for epoch #  50 ==============>
test batch for epoch #  50 ======================>
Scheduler is currently registering the learning rate.
Epoch 50, Pointwise Training loss 0.3289048879617645, Pointwise Validation loss 0.2970404376586278
Full training loss: 40.7842061072588, Full test loss: 3.564485251903534
The current learning rate is: 0.000125
Epoch 51 started ======>
train batch for epoch #  51 ==============>
test batch for epoch #  51 ======================>
Scheduler is currently registering the learning rate.
Epoch 51, Pointwise Training loss 0.32889836077247897, Pointwise Validation loss 0.2934828996658325
Full training loss: 40.78339673578739, Full test loss: 3.5217947959899902
The current learning rate is: 0.000125
Epoch 52 started ======>
train batch for epoch #  52 ==============>
test batch for epoch #  52 ======================>
Scheduler is currently registering the learning rate.
Epoch 52, Pointwise Training loss 0.32944630783411766, Pointwise Validation loss 0.296847902238369
Full training loss: 40.85134217143059, Full test loss: 3.562174826860428
The current learning rate is: 6.25e-05
Epoch 53 started ======>
train batch for epoch #  53 ==============>
test batch for epoch #  53 ======================>
Scheduler is currently registering the learning rate.
Epoch 53, Pointwise Training loss 0.32907647939939655, Pointwise Validation loss 0.2954971392949422
Full training loss: 40.80548344552517, Full test loss: 3.5459656715393066
The current learning rate is: 6.25e-05
Epoch 54 started ======>
train batch for epoch #  54 ==============>
test batch for epoch #  54 ======================>
Scheduler is currently registering the learning rate.
Epoch 54, Pointwise Training loss 0.32893894841113397, Pointwise Validation loss 0.2953698808948199
Full training loss: 40.788429602980614, Full test loss: 3.5444385707378387
The current learning rate is: 6.25e-05
Epoch 55 started ======>
train batch for epoch #  55 ==============>
test batch for epoch #  55 ======================>
Scheduler is currently registering the learning rate.
Epoch 55, Pointwise Training loss 0.3288823743501017, Pointwise Validation loss 0.2962546373407046
Full training loss: 40.78141441941261, Full test loss: 3.555055648088455
The current learning rate is: 6.25e-05
Epoch 56 started ======>
train batch for epoch #  56 ==============>
test batch for epoch #  56 ======================>
Scheduler is currently registering the learning rate.
Epoch 56, Pointwise Training loss 0.32895523189536985, Pointwise Validation loss 0.2940795918305715
Full training loss: 40.790448755025864, Full test loss: 3.528955101966858
The current learning rate is: 6.25e-05
Epoch 57 started ======>
train batch for epoch #  57 ==============>
test batch for epoch #  57 ======================>
Scheduler is currently registering the learning rate.
Epoch 57, Pointwise Training loss 0.32894153292140654, Pointwise Validation loss 0.29346751421689987
Full training loss: 40.78875008225441, Full test loss: 3.5216101706027985
The current learning rate is: 6.25e-05
Epoch 58 started ======>
train batch for epoch #  58 ==============>
test batch for epoch #  58 ======================>
Scheduler is currently registering the learning rate.
Epoch 58, Pointwise Training loss 0.35226123239244184, Pointwise Validation loss 0.3193894997239113
Full training loss: 43.68039281666279, Full test loss: 3.8326739966869354
The current learning rate is: 6.25e-05
Epoch 59 started ======>
train batch for epoch #  59 ==============>
test batch for epoch #  59 ======================>
Scheduler is currently registering the learning rate.
Epoch 59, Pointwise Training loss 0.3679735403387777, Pointwise Validation loss 0.32267439365386963
Full training loss: 45.62871900200844, Full test loss: 3.8720927238464355
The current learning rate is: 6.25e-05
Epoch 60 started ======>
train batch for epoch #  60 ==============>
test batch for epoch #  60 ======================>
Scheduler is currently registering the learning rate.
Epoch 60, Pointwise Training loss 0.367916579688749, Pointwise Validation loss 0.3208969632784526
Full training loss: 45.62165588140488, Full test loss: 3.8507635593414307
The current learning rate is: 6.25e-05
Epoch 61 started ======>
train batch for epoch #  61 ==============>
test batch for epoch #  61 ======================>
Scheduler is currently registering the learning rate.
Epoch 61, Pointwise Training loss 0.3678002511301348, Pointwise Validation loss 0.32280173152685165
Full training loss: 45.60723114013672, Full test loss: 3.87362077832222
The current learning rate is: 6.25e-05
Epoch 62 started ======>
train batch for epoch #  62 ==============>
test batch for epoch #  62 ======================>
Scheduler is currently registering the learning rate.
Epoch 62, Pointwise Training loss 0.3677104862947618, Pointwise Validation loss 0.32237962633371353
Full training loss: 45.59610030055046, Full test loss: 3.8685555160045624
The current learning rate is: 6.25e-05
Epoch 63 started ======>
train batch for epoch #  63 ==============>
test batch for epoch #  63 ======================>
Scheduler is currently registering the learning rate.
Epoch 63, Pointwise Training loss 0.36767477277786503, Pointwise Validation loss 0.32438262552022934
Full training loss: 45.59167182445526, Full test loss: 3.892591506242752
The current learning rate is: 3.125e-05
Epoch 64 started ======>
train batch for epoch #  64 ==============>
test batch for epoch #  64 ======================>
Scheduler is currently registering the learning rate.
Epoch 64, Pointwise Training loss 0.367946955946184, Pointwise Validation loss 0.32346229006846744
Full training loss: 45.62542253732681, Full test loss: 3.8815474808216095
The current learning rate is: 3.125e-05
Epoch 65 started ======>
train batch for epoch #  65 ==============>
test batch for epoch #  65 ======================>
Scheduler is currently registering the learning rate.
Epoch 65, Pointwise Training loss 0.3675919369824471, Pointwise Validation loss 0.322245533267657
Full training loss: 45.58140018582344, Full test loss: 3.8669463992118835
The current learning rate is: 3.125e-05
Epoch 66 started ======>
train batch for epoch #  66 ==============>
test batch for epoch #  66 ======================>
Scheduler is currently registering the learning rate.
Epoch 66, Pointwise Training loss 0.36806762554953176, Pointwise Validation loss 0.3226998746395111
Full training loss: 45.64038556814194, Full test loss: 3.8723984956741333
The current learning rate is: 3.125e-05
Epoch 67 started ======>
train batch for epoch #  67 ==============>
test batch for epoch #  67 ======================>
Scheduler is currently registering the learning rate.
Epoch 67, Pointwise Training loss 0.36799782274230836, Pointwise Validation loss 0.3217004214723905
Full training loss: 45.631730020046234, Full test loss: 3.860405057668686
The current learning rate is: 3.125e-05
Epoch 68 started ======>
train batch for epoch #  68 ==============>
test batch for epoch #  68 ======================>
Scheduler is currently registering the learning rate.
Epoch 68, Pointwise Training loss 0.3677712758221934, Pointwise Validation loss 0.32283228635787964
Full training loss: 45.60363820195198, Full test loss: 3.8739874362945557
The current learning rate is: 3.125e-05
Epoch 69 started ======>
train batch for epoch #  69 ==============>
test batch for epoch #  69 ======================>
Scheduler is currently registering the learning rate.
Epoch 69, Pointwise Training loss 0.36757068504248896, Pointwise Validation loss 0.3225067953268687
Full training loss: 45.57876494526863, Full test loss: 3.8700815439224243
The current learning rate is: 3.125e-05
Epoch 70 started ======>
train batch for epoch #  70 ==============>
test batch for epoch #  70 ======================>
Scheduler is currently registering the learning rate.
Epoch 70, Pointwise Training loss 0.3675195300771344, Pointwise Validation loss 0.3232726529240608
Full training loss: 45.57242172956467, Full test loss: 3.87927183508873
The current learning rate is: 3.125e-05
Epoch 71 started ======>
train batch for epoch #  71 ==============>
test batch for epoch #  71 ======================>
Scheduler is currently registering the learning rate.
Epoch 71, Pointwise Training loss 0.36765553994524863, Pointwise Validation loss 0.3224076181650162
Full training loss: 45.58928695321083, Full test loss: 3.868891417980194
The current learning rate is: 3.125e-05
Epoch 72 started ======>
train batch for epoch #  72 ==============>
test batch for epoch #  72 ======================>
Scheduler is currently registering the learning rate.
Epoch 72, Pointwise Training loss 0.3678098308943933, Pointwise Validation loss 0.3234220047791799
Full training loss: 45.60841903090477, Full test loss: 3.8810640573501587
The current learning rate is: 3.125e-05
Epoch 73 started ======>
train batch for epoch #  73 ==============>
test batch for epoch #  73 ======================>
Scheduler is currently registering the learning rate.
Epoch 73, Pointwise Training loss 0.36807426473786753, Pointwise Validation loss 0.323328398168087
Full training loss: 45.641208827495575, Full test loss: 3.879940778017044
The current learning rate is: 3.125e-05
Epoch 74 started ======>
train batch for epoch #  74 ==============>
test batch for epoch #  74 ======================>
Scheduler is currently registering the learning rate.
Epoch 74, Pointwise Training loss 0.36771812073646054, Pointwise Validation loss 0.3222238098581632
Full training loss: 45.597046971321106, Full test loss: 3.8666857182979584
The current learning rate is: 1.5625e-05
Epoch 75 started ======>
train batch for epoch #  75 ==============>
test batch for epoch #  75 ======================>
Scheduler is currently registering the learning rate.
Epoch 75, Pointwise Training loss 0.367790411797262, Pointwise Validation loss 0.32275446007649106
Full training loss: 45.60601106286049, Full test loss: 3.8730535209178925
The current learning rate is: 1.5625e-05
Epoch 76 started ======>
train batch for epoch #  76 ==============>
test batch for epoch #  76 ======================>
Scheduler is currently registering the learning rate.
Epoch 76, Pointwise Training loss 0.36800613062035653, Pointwise Validation loss 0.32301508138577145
Full training loss: 45.63276019692421, Full test loss: 3.876180976629257
The current learning rate is: 1.5625e-05
Epoch 77 started ======>
train batch for epoch #  77 ==============>
test batch for epoch #  77 ======================>
Scheduler is currently registering the learning rate.
Epoch 77, Pointwise Training loss 0.367652352538801, Pointwise Validation loss 0.324060079952081
Full training loss: 45.588891714811325, Full test loss: 3.8887209594249725
The current learning rate is: 1.5625e-05
Epoch 78 started ======>
train batch for epoch #  78 ==============>
test batch for epoch #  78 ======================>
Scheduler is currently registering the learning rate.
Epoch 78, Pointwise Training loss 0.36772955353221587, Pointwise Validation loss 0.3222464770078659
Full training loss: 45.598464637994766, Full test loss: 3.866957724094391
The current learning rate is: 1.5625e-05
Epoch 79 started ======>
train batch for epoch #  79 ==============>
test batch for epoch #  79 ======================>
Scheduler is currently registering the learning rate.
Epoch 79, Pointwise Training loss 0.36789617278883535, Pointwise Validation loss 0.3225433975458145
Full training loss: 45.61912542581558, Full test loss: 3.870520770549774
The current learning rate is: 1.5625e-05
Epoch 80 started ======>
train batch for epoch #  80 ==============>
test batch for epoch #  80 ======================>
Scheduler is currently registering the learning rate.
Epoch 80, Pointwise Training loss 0.3678559653701321, Pointwise Validation loss 0.322404441734155
Full training loss: 45.61413970589638, Full test loss: 3.8688533008098602
The current learning rate is: 1.5625e-05
Epoch 81 started ======>
train batch for epoch #  81 ==============>
test batch for epoch #  81 ======================>
Scheduler is currently registering the learning rate.
Epoch 81, Pointwise Training loss 0.36743996436557463, Pointwise Validation loss 0.32254789769649506
Full training loss: 45.56255558133125, Full test loss: 3.8705747723579407
The current learning rate is: 1.5625e-05
Epoch 82 started ======>
train batch for epoch #  82 ==============>
test batch for epoch #  82 ======================>
Scheduler is currently registering the learning rate.
Epoch 82, Pointwise Training loss 0.36799470094903824, Pointwise Validation loss 0.32285449902216595
Full training loss: 45.63134291768074, Full test loss: 3.874253988265991
The current learning rate is: 1.5625e-05
Epoch 83 started ======>
train batch for epoch #  83 ==============>
test batch for epoch #  83 ======================>
Scheduler is currently registering the learning rate.
Epoch 83, Pointwise Training loss 0.3676288267297129, Pointwise Validation loss 0.32283621778090793
Full training loss: 45.585974514484406, Full test loss: 3.8740346133708954
The current learning rate is: 1.5625e-05
Epoch 84 started ======>
train batch for epoch #  84 ==============>
test batch for epoch #  84 ======================>
Scheduler is currently registering the learning rate.
Epoch 84, Pointwise Training loss 0.36792901855322624, Pointwise Validation loss 0.3229867219924927
Full training loss: 45.62319830060005, Full test loss: 3.875840663909912
The current learning rate is: 1.5625e-05
Epoch 85 started ======>
train batch for epoch #  85 ==============>
test batch for epoch #  85 ======================>
Scheduler is currently registering the learning rate.
Epoch 85, Pointwise Training loss 0.36745174541588754, Pointwise Validation loss 0.32280636578798294
Full training loss: 45.56401643157005, Full test loss: 3.8736763894557953
The current learning rate is: 7.8125e-06
Epoch 86 started ======>
train batch for epoch #  86 ==============>
test batch for epoch #  86 ======================>
Scheduler is currently registering the learning rate.
Epoch 86, Pointwise Training loss 0.3681864762498486, Pointwise Validation loss 0.32279109954833984
Full training loss: 45.65512305498123, Full test loss: 3.873493194580078
The current learning rate is: 7.8125e-06
Epoch 87 started ======>
train batch for epoch #  87 ==============>
test batch for epoch #  87 ======================>
Scheduler is currently registering the learning rate.
Epoch 87, Pointwise Training loss 0.36817886872637656, Pointwise Validation loss 0.3224041362603505
Full training loss: 45.654179722070694, Full test loss: 3.8688496351242065
The current learning rate is: 7.8125e-06
Epoch 88 started ======>
train batch for epoch #  88 ==============>
test batch for epoch #  88 ======================>
Scheduler is currently registering the learning rate.
Epoch 88, Pointwise Training loss 0.36740855920699333, Pointwise Validation loss 0.32277113447586697
Full training loss: 45.558661341667175, Full test loss: 3.8732536137104034
The current learning rate is: 7.8125e-06
Epoch 89 started ======>
train batch for epoch #  89 ==============>
test batch for epoch #  89 ======================>
Scheduler is currently registering the learning rate.
Epoch 89, Pointwise Training loss 0.3678071383507021, Pointwise Validation loss 0.3227747231721878
Full training loss: 45.60808515548706, Full test loss: 3.8732966780662537
The current learning rate is: 7.8125e-06
Epoch 90 started ======>
train batch for epoch #  90 ==============>
test batch for epoch #  90 ======================>
Scheduler is currently registering the learning rate.
Epoch 90, Pointwise Training loss 0.3675196516898371, Pointwise Validation loss 0.3226286992430687
Full training loss: 45.572436809539795, Full test loss: 3.8715443909168243
The current learning rate is: 7.8125e-06
Epoch 91 started ======>
train batch for epoch #  91 ==============>
test batch for epoch #  91 ======================>
Scheduler is currently registering the learning rate.
Epoch 91, Pointwise Training loss 0.36741218139087, Pointwise Validation loss 0.3226364254951477
Full training loss: 45.55911049246788, Full test loss: 3.8716371059417725
The current learning rate is: 7.8125e-06
Epoch 92 started ======>
train batch for epoch #  92 ==============>
test batch for epoch #  92 ======================>
Scheduler is currently registering the learning rate.
Epoch 92, Pointwise Training loss 0.3675001568851932, Pointwise Validation loss 0.3225954944888751
Full training loss: 45.57001945376396, Full test loss: 3.871145933866501
The current learning rate is: 7.8125e-06
Epoch 93 started ======>
train batch for epoch #  93 ==============>
test batch for epoch #  93 ======================>
Scheduler is currently registering the learning rate.
Epoch 93, Pointwise Training loss 0.3674674324931637, Pointwise Validation loss 0.3227202817797661
Full training loss: 45.5659616291523, Full test loss: 3.872643381357193
The current learning rate is: 7.8125e-06
Epoch 94 started ======>
train batch for epoch #  94 ==============>
test batch for epoch #  94 ======================>
Scheduler is currently registering the learning rate.
Epoch 94, Pointwise Training loss 0.36792035208594415, Pointwise Validation loss 0.32256683707237244
Full training loss: 45.622123658657074, Full test loss: 3.8708020448684692
The current learning rate is: 7.8125e-06
Epoch 95 started ======>
train batch for epoch #  95 ==============>
test batch for epoch #  95 ======================>
Scheduler is currently registering the learning rate.
Epoch 95, Pointwise Training loss 0.3678030186603146, Pointwise Validation loss 0.3222483495871226
Full training loss: 45.60757431387901, Full test loss: 3.866980195045471
The current learning rate is: 7.8125e-06
Epoch 96 started ======>
train batch for epoch #  96 ==============>
test batch for epoch #  96 ======================>
Scheduler is currently registering the learning rate.
Epoch 96, Pointwise Training loss 0.36775942939904427, Pointwise Validation loss 0.32241961856683093
Full training loss: 45.60216924548149, Full test loss: 3.8690354228019714
The current learning rate is: 3.90625e-06
Epoch 97 started ======>
train batch for epoch #  97 ==============>
test batch for epoch #  97 ======================>
Scheduler is currently registering the learning rate.
Epoch 97, Pointwise Training loss 0.36744381174925833, Pointwise Validation loss 0.32270968208710354
Full training loss: 45.563032656908035, Full test loss: 3.8725161850452423
The current learning rate is: 3.90625e-06
Epoch 98 started ======>
train batch for epoch #  98 ==============>
test batch for epoch #  98 ======================>
Scheduler is currently registering the learning rate.
Epoch 98, Pointwise Training loss 0.3677084222435951, Pointwise Validation loss 0.32263974597056705
Full training loss: 45.595844358205795, Full test loss: 3.871676951646805
The current learning rate is: 3.90625e-06
Epoch 99 started ======>
train batch for epoch #  99 ==============>
test batch for epoch #  99 ======================>
Scheduler is currently registering the learning rate.
Epoch 99, Pointwise Training loss 0.3674145744692895, Pointwise Validation loss 0.32251856724421185
Full training loss: 45.559407234191895, Full test loss: 3.870222806930542
The current learning rate is: 3.90625e-06
Epoch 100 started ======>
train batch for epoch #  100 ==============>
test batch for epoch #  100 ======================>
Scheduler is currently registering the learning rate.
Epoch 100, Pointwise Training loss 0.3675998713701002, Pointwise Validation loss 0.3226641391714414
Full training loss: 45.582384049892426, Full test loss: 3.8719696700572968
The current learning rate is: 3.90625e-06
Epoch 101 started ======>
train batch for epoch #  101 ==============>
test batch for epoch #  101 ======================>
Scheduler is currently registering the learning rate.
Epoch 101, Pointwise Training loss 0.367459939372155, Pointwise Validation loss 0.32258909195661545
Full training loss: 45.56503248214722, Full test loss: 3.8710691034793854
The current learning rate is: 3.90625e-06
Epoch 102 started ======>
train batch for epoch #  102 ==============>
test batch for epoch #  102 ======================>
Scheduler is currently registering the learning rate.
Epoch 102, Pointwise Training loss 0.3681419109625201, Pointwise Validation loss 0.3226148535807927
Full training loss: 45.64959695935249, Full test loss: 3.871378242969513
The current learning rate is: 3.90625e-06
Epoch 103 started ======>
train batch for epoch #  103 ==============>
test batch for epoch #  103 ======================>
Scheduler is currently registering the learning rate.
Epoch 103, Pointwise Training loss 0.36763945870822473, Pointwise Validation loss 0.32276255389054614
Full training loss: 45.58729287981987, Full test loss: 3.873150646686554
The current learning rate is: 3.90625e-06
Epoch 104 started ======>
train batch for epoch #  104 ==============>
test batch for epoch #  104 ======================>
Scheduler is currently registering the learning rate.
Epoch 104, Pointwise Training loss 0.36747910707227643, Pointwise Validation loss 0.32267002016305923
Full training loss: 45.56740927696228, Full test loss: 3.872040241956711
The current learning rate is: 3.90625e-06
Epoch 105 started ======>
train batch for epoch #  105 ==============>
test batch for epoch #  105 ======================>
Scheduler is currently registering the learning rate.
Epoch 105, Pointwise Training loss 0.36743745087615903, Pointwise Validation loss 0.32259751111268997
Full training loss: 45.56224390864372, Full test loss: 3.8711701333522797
The current learning rate is: 3.90625e-06
Epoch 106 started ======>
train batch for epoch #  106 ==============>
test batch for epoch #  106 ======================>
Scheduler is currently registering the learning rate.
Epoch 106, Pointwise Training loss 0.36770291217873174, Pointwise Validation loss 0.32264044632514316
Full training loss: 45.595161110162735, Full test loss: 3.871685355901718
The current learning rate is: 3.90625e-06
Epoch 107 started ======>
train batch for epoch #  107 ==============>
test batch for epoch #  107 ======================>
Scheduler is currently registering the learning rate.
Epoch 107, Pointwise Training loss 0.3680294228176917, Pointwise Validation loss 0.3225114047527313
Full training loss: 45.63564842939377, Full test loss: 3.870136857032776
The current learning rate is: 1.953125e-06
Epoch 108 started ======>
train batch for epoch #  108 ==============>
test batch for epoch #  108 ======================>
Scheduler is currently registering the learning rate.
Epoch 108, Pointwise Training loss 0.3674284116395058, Pointwise Validation loss 0.32255913813908893
Full training loss: 45.56112304329872, Full test loss: 3.8707096576690674
The current learning rate is: 1.953125e-06
Epoch 109 started ======>
train batch for epoch #  109 ==============>
test batch for epoch #  109 ======================>
Scheduler is currently registering the learning rate.
Epoch 109, Pointwise Training loss 0.36761437404540276, Pointwise Validation loss 0.32250917206207913
Full training loss: 45.584182381629944, Full test loss: 3.8701100647449493
The current learning rate is: 1.953125e-06
Epoch 110 started ======>
train batch for epoch #  110 ==============>
test batch for epoch #  110 ======================>
Scheduler is currently registering the learning rate.
Epoch 110, Pointwise Training loss 0.36748326665932135, Pointwise Validation loss 0.3225514863928159
Full training loss: 45.567925065755844, Full test loss: 3.870617836713791
The current learning rate is: 1.953125e-06
Epoch 111 started ======>
train batch for epoch #  111 ==============>
test batch for epoch #  111 ======================>
Scheduler is currently registering the learning rate.
Epoch 111, Pointwise Training loss 0.3678757002757442, Pointwise Validation loss 0.3224601323405902
Full training loss: 45.616586834192276, Full test loss: 3.869521588087082
The current learning rate is: 1.953125e-06
Epoch 112 started ======>
train batch for epoch #  112 ==============>
test batch for epoch #  112 ======================>
Scheduler is currently registering the learning rate.
Epoch 112, Pointwise Training loss 0.3676147891148444, Pointwise Validation loss 0.3224458768963814
Full training loss: 45.58423385024071, Full test loss: 3.8693505227565765
The current learning rate is: 1.953125e-06
Epoch 113 started ======>
train batch for epoch #  113 ==============>
test batch for epoch #  113 ======================>
Scheduler is currently registering the learning rate.
Epoch 113, Pointwise Training loss 0.36779963946150196, Pointwise Validation loss 0.3225279649098714
Full training loss: 45.60715529322624, Full test loss: 3.870335578918457
The current learning rate is: 1.953125e-06
Epoch 114 started ======>
train batch for epoch #  114 ==============>
test batch for epoch #  114 ======================>
Scheduler is currently registering the learning rate.
Epoch 114, Pointwise Training loss 0.3677125813499574, Pointwise Validation loss 0.3227398196856181
Full training loss: 45.596360087394714, Full test loss: 3.872877836227417
The current learning rate is: 1.953125e-06
Epoch 115 started ======>
train batch for epoch #  115 ==============>
test batch for epoch #  115 ======================>
Scheduler is currently registering the learning rate.
Epoch 115, Pointwise Training loss 0.367808737581776, Pointwise Validation loss 0.322616013387839
Full training loss: 45.60828346014023, Full test loss: 3.871392160654068
The current learning rate is: 1.953125e-06
Epoch 116 started ======>
train batch for epoch #  116 ==============>
test batch for epoch #  116 ======================>
Scheduler is currently registering the learning rate.
Epoch 116, Pointwise Training loss 0.367861635021625, Pointwise Validation loss 0.3225252752502759
Full training loss: 45.6148427426815, Full test loss: 3.870303303003311
The current learning rate is: 1.953125e-06
Epoch 117 started ======>
train batch for epoch #  117 ==============>
test batch for epoch #  117 ======================>
Scheduler is currently registering the learning rate.
Epoch 117, Pointwise Training loss 0.36750557225558067, Pointwise Validation loss 0.32253287235895794
Full training loss: 45.570690959692, Full test loss: 3.870394468307495
The current learning rate is: 1.953125e-06
Epoch 118 started ======>
train batch for epoch #  118 ==============>
test batch for epoch #  118 ======================>
Scheduler is currently registering the learning rate.
Epoch 118, Pointwise Training loss 0.3673621937632561, Pointwise Validation loss 0.3225981717308362
Full training loss: 45.55291202664375, Full test loss: 3.871178060770035
The current learning rate is: 9.765625e-07
Epoch 119 started ======>
train batch for epoch #  119 ==============>
test batch for epoch #  119 ======================>
Scheduler is currently registering the learning rate.
Epoch 119, Pointwise Training loss 0.3681364987165697, Pointwise Validation loss 0.322541003425916
Full training loss: 45.648925840854645, Full test loss: 3.8704920411109924
The current learning rate is: 9.765625e-07
Epoch 120 started ======>
train batch for epoch #  120 ==============>
test batch for epoch #  120 ======================>
Scheduler is currently registering the learning rate.
Epoch 120, Pointwise Training loss 0.3677011579275131, Pointwise Validation loss 0.32257456332445145
Full training loss: 45.59494358301163, Full test loss: 3.8708947598934174
The current learning rate is: 9.765625e-07
Epoch 121 started ======>
train batch for epoch #  121 ==============>
test batch for epoch #  121 ======================>
Scheduler is currently registering the learning rate.
Epoch 121, Pointwise Training loss 0.36768433595857314, Pointwise Validation loss 0.3226097250978152
Full training loss: 45.59285765886307, Full test loss: 3.8713167011737823
The current learning rate is: 9.765625e-07
Epoch 122 started ======>
train batch for epoch #  122 ==============>
test batch for epoch #  122 ======================>
Scheduler is currently registering the learning rate.
Epoch 122, Pointwise Training loss 0.3679304634851794, Pointwise Validation loss 0.32257404426733655
Full training loss: 45.62337747216225, Full test loss: 3.8708885312080383
The current learning rate is: 9.765625e-07
Epoch 123 started ======>
train batch for epoch #  123 ==============>
test batch for epoch #  123 ======================>
Scheduler is currently registering the learning rate.
Epoch 123, Pointwise Training loss 0.36797006620514777, Pointwise Validation loss 0.32262390355269116
Full training loss: 45.628288209438324, Full test loss: 3.8714868426322937
The current learning rate is: 9.765625e-07
Epoch 124 started ======>
train batch for epoch #  124 ==============>
test batch for epoch #  124 ======================>
Scheduler is currently registering the learning rate.
Epoch 124, Pointwise Training loss 0.36808651493441674, Pointwise Validation loss 0.3226246312260628
Full training loss: 45.642727851867676, Full test loss: 3.8714955747127533
The current learning rate is: 9.765625e-07
Epoch 125 started ======>
train batch for epoch #  125 ==============>
test batch for epoch #  125 ======================>
Scheduler is currently registering the learning rate.
Epoch 125, Pointwise Training loss 0.36755493403442446, Pointwise Validation loss 0.32259073108434677
Full training loss: 45.57681182026863, Full test loss: 3.8710887730121613
The current learning rate is: 9.765625e-07
Epoch 126 started ======>
train batch for epoch #  126 ==============>
test batch for epoch #  126 ======================>
Scheduler is currently registering the learning rate.
Epoch 126, Pointwise Training loss 0.36786654519457973, Pointwise Validation loss 0.32259875535964966
Full training loss: 45.615451604127884, Full test loss: 3.871185064315796
The current learning rate is: 9.765625e-07
Epoch 127 started ======>
train batch for epoch #  127 ==============>
test batch for epoch #  127 ======================>
Scheduler is currently registering the learning rate.
Epoch 127, Pointwise Training loss 0.36766007975224524, Pointwise Validation loss 0.3225616191824277
Full training loss: 45.58984988927841, Full test loss: 3.8707394301891327
The current learning rate is: 9.765625e-07
Epoch 128 started ======>
train batch for epoch #  128 ==============>
test batch for epoch #  128 ======================>
Scheduler is currently registering the learning rate.
Epoch 128, Pointwise Training loss 0.3678100719567268, Pointwise Validation loss 0.3226105148593585
Full training loss: 45.608448922634125, Full test loss: 3.8713261783123016
The current learning rate is: 9.765625e-07
Epoch 129 started ======>
train batch for epoch #  129 ==============>
test batch for epoch #  129 ======================>
Scheduler is currently registering the learning rate.
Epoch 129, Pointwise Training loss 0.36783670057212153, Pointwise Validation loss 0.32255324969689053
Full training loss: 45.61175087094307, Full test loss: 3.870638996362686
The current learning rate is: 4.8828125e-07
Epoch 130 started ======>
train batch for epoch #  130 ==============>
test batch for epoch #  130 ======================>
Scheduler is currently registering the learning rate.
Epoch 130, Pointwise Training loss 0.3675643498859098, Pointwise Validation loss 0.3225700507561366
Full training loss: 45.577979385852814, Full test loss: 3.870840609073639
The current learning rate is: 4.8828125e-07
Epoch 131 started ======>
train batch for epoch #  131 ==============>
test batch for epoch #  131 ======================>
Scheduler is currently registering the learning rate.
Epoch 131, Pointwise Training loss 0.367694272389335, Pointwise Validation loss 0.32258230944474536
Full training loss: 45.59408977627754, Full test loss: 3.8709877133369446
The current learning rate is: 4.8828125e-07
Epoch 132 started ======>
train batch for epoch #  132 ==============>
test batch for epoch #  132 ======================>
Scheduler is currently registering the learning rate.
Epoch 132, Pointwise Training loss 0.3676742055723744, Pointwise Validation loss 0.32256758213043213
Full training loss: 45.591601490974426, Full test loss: 3.8708109855651855
The current learning rate is: 4.8828125e-07
Epoch 133 started ======>
train batch for epoch #  133 ==============>
test batch for epoch #  133 ======================>
Scheduler is currently registering the learning rate.
Epoch 133, Pointwise Training loss 0.36763773666274163, Pointwise Validation loss 0.32260762155056
Full training loss: 45.58707934617996, Full test loss: 3.87129145860672
The current learning rate is: 4.8828125e-07
Epoch 134 started ======>
train batch for epoch #  134 ==============>
test batch for epoch #  134 ======================>
Scheduler is currently registering the learning rate.
Epoch 134, Pointwise Training loss 0.36766207146067775, Pointwise Validation loss 0.3225862408677737
Full training loss: 45.59009686112404, Full test loss: 3.8710348904132843
The current learning rate is: 4.8828125e-07
Epoch 135 started ======>
train batch for epoch #  135 ==============>
test batch for epoch #  135 ======================>
Scheduler is currently registering the learning rate.
Epoch 135, Pointwise Training loss 0.36747004019637264, Pointwise Validation loss 0.3225892161329587
Full training loss: 45.566284984350204, Full test loss: 3.8710705935955048
The current learning rate is: 4.8828125e-07
Epoch 136 started ======>
train batch for epoch #  136 ==============>
test batch for epoch #  136 ======================>
Scheduler is currently registering the learning rate.
Epoch 136, Pointwise Training loss 0.3681645256377036, Pointwise Validation loss 0.32259595145781833
Full training loss: 45.65240117907524, Full test loss: 3.87115141749382
The current learning rate is: 4.8828125e-07
Epoch 137 started ======>
train batch for epoch #  137 ==============>
test batch for epoch #  137 ======================>
Scheduler is currently registering the learning rate.
Epoch 137, Pointwise Training loss 0.3677180577670374, Pointwise Validation loss 0.32262423634529114
Full training loss: 45.59703916311264, Full test loss: 3.8714908361434937
The current learning rate is: 4.8828125e-07
Epoch 138 started ======>
train batch for epoch #  138 ==============>
test batch for epoch #  138 ======================>
Scheduler is currently registering the learning rate.
Epoch 138, Pointwise Training loss 0.36750461089034236, Pointwise Validation loss 0.3225901201367378
Full training loss: 45.57057175040245, Full test loss: 3.871081441640854
The current learning rate is: 4.8828125e-07
Epoch 139 started ======>
train batch for epoch #  139 ==============>
test batch for epoch #  139 ======================>
Scheduler is currently registering the learning rate.
Epoch 139, Pointwise Training loss 0.36748867481946945, Pointwise Validation loss 0.3225775559743245
Full training loss: 45.56859567761421, Full test loss: 3.8709306716918945
The current learning rate is: 4.8828125e-07
Epoch 140 started ======>
train batch for epoch #  140 ==============>
test batch for epoch #  140 ======================>
Scheduler is currently registering the learning rate.
Epoch 140, Pointwise Training loss 0.3681039254992239, Pointwise Validation loss 0.32260630776484805
Full training loss: 45.64488676190376, Full test loss: 3.871275693178177
The current learning rate is: 2.44140625e-07
Epoch 141 started ======>
train batch for epoch #  141 ==============>
test batch for epoch #  141 ======================>
Scheduler is currently registering the learning rate.
Epoch 141, Pointwise Training loss 0.36770850011417944, Pointwise Validation loss 0.3226047207911809
Full training loss: 45.59585401415825, Full test loss: 3.871256649494171
The current learning rate is: 2.44140625e-07
Epoch 142 started ======>
train batch for epoch #  142 ==============>
test batch for epoch #  142 ======================>
Scheduler is currently registering the learning rate.
Epoch 142, Pointwise Training loss 0.36831776677600797, Pointwise Validation loss 0.32258949677149457
Full training loss: 45.67140308022499, Full test loss: 3.8710739612579346
The current learning rate is: 2.44140625e-07
Epoch 143 started ======>
train batch for epoch #  143 ==============>
test batch for epoch #  143 ======================>
Scheduler is currently registering the learning rate.
Epoch 143, Pointwise Training loss 0.36756131028936756, Pointwise Validation loss 0.32261886696020764
Full training loss: 45.57760247588158, Full test loss: 3.8714264035224915
The current learning rate is: 2.44140625e-07
Epoch 144 started ======>
train batch for epoch #  144 ==============>
test batch for epoch #  144 ======================>
Scheduler is currently registering the learning rate.
Epoch 144, Pointwise Training loss 0.367795902154138, Pointwise Validation loss 0.322599063316981
Full training loss: 45.60669186711311, Full test loss: 3.871188759803772
The current learning rate is: 2.44140625e-07
Epoch 145 started ======>
train batch for epoch #  145 ==============>
test batch for epoch #  145 ======================>
Scheduler is currently registering the learning rate.
Epoch 145, Pointwise Training loss 0.367944766677195, Pointwise Validation loss 0.3225833500425021
Full training loss: 45.62515106797218, Full test loss: 3.871000200510025
The current learning rate is: 2.44140625e-07
Epoch 146 started ======>
train batch for epoch #  146 ==============>
test batch for epoch #  146 ======================>
Scheduler is currently registering the learning rate.
Epoch 146, Pointwise Training loss 0.3676038913188442, Pointwise Validation loss 0.32258179287115735
Full training loss: 45.58288252353668, Full test loss: 3.870981514453888
The current learning rate is: 2.44140625e-07
Epoch 147 started ======>
train batch for epoch #  147 ==============>
test batch for epoch #  147 ======================>
Scheduler is currently registering the learning rate.
Epoch 147, Pointwise Training loss 0.3679807938395008, Pointwise Validation loss 0.32259368399779004
Full training loss: 45.6296184360981, Full test loss: 3.8711242079734802
The current learning rate is: 2.44140625e-07
Epoch 148 started ======>
train batch for epoch #  148 ==============>
test batch for epoch #  148 ======================>
Scheduler is currently registering the learning rate.
Epoch 148, Pointwise Training loss 0.36804558913553914, Pointwise Validation loss 0.32258557031552
Full training loss: 45.637653052806854, Full test loss: 3.8710268437862396
The current learning rate is: 2.44140625e-07
Epoch 149 started ======>
train batch for epoch #  149 ==============>
test batch for epoch #  149 ======================>
Scheduler is currently registering the learning rate.
Epoch 149, Pointwise Training loss 0.36763934021995914, Pointwise Validation loss 0.32260243346293765
Full training loss: 45.58727818727493, Full test loss: 3.871229201555252
The current learning rate is: 2.44140625e-07
Epoch 150 started ======>
train batch for epoch #  150 ==============>
test batch for epoch #  150 ======================>
Scheduler is currently registering the learning rate.
Epoch 150, Pointwise Training loss 0.36771470091996655, Pointwise Validation loss 0.32259275515874225
Full training loss: 45.59662291407585, Full test loss: 3.8711130619049072
The current learning rate is: 2.44140625e-07
Epoch 151 started ======>
train batch for epoch #  151 ==============>
test batch for epoch #  151 ======================>
Scheduler is currently registering the learning rate.
Epoch 151, Pointwise Training loss 0.36782477219258586, Pointwise Validation loss 0.32259565343459445
Full training loss: 45.610271751880646, Full test loss: 3.8711478412151337
The current learning rate is: 1.220703125e-07
Epoch 152 started ======>
train batch for epoch #  152 ==============>
test batch for epoch #  152 ======================>
Scheduler is currently registering the learning rate.
Epoch 152, Pointwise Training loss 0.36794185422120557, Pointwise Validation loss 0.32259275019168854
Full training loss: 45.62478992342949, Full test loss: 3.8711130023002625
The current learning rate is: 1.220703125e-07
Epoch 153 started ======>
train batch for epoch #  153 ==============>
test batch for epoch #  153 ======================>
Scheduler is currently registering the learning rate.
Epoch 153, Pointwise Training loss 0.3677118146611798, Pointwise Validation loss 0.322592260936896
Full training loss: 45.5962650179863, Full test loss: 3.871107131242752
The current learning rate is: 1.220703125e-07
Epoch 154 started ======>
train batch for epoch #  154 ==============>
test batch for epoch #  154 ======================>
Scheduler is currently registering the learning rate.
Epoch 154, Pointwise Training loss 0.36766845564688405, Pointwise Validation loss 0.3226064244906108
Full training loss: 45.59088850021362, Full test loss: 3.871277093887329
The current learning rate is: 1.220703125e-07
Epoch 155 started ======>
train batch for epoch #  155 ==============>
test batch for epoch #  155 ======================>
Scheduler is currently registering the learning rate.
Epoch 155, Pointwise Training loss 0.3678241994592451, Pointwise Validation loss 0.3225916177034378
Full training loss: 45.610200732946396, Full test loss: 3.8710994124412537
The current learning rate is: 1.220703125e-07
Epoch 156 started ======>
train batch for epoch #  156 ==============>
test batch for epoch #  156 ======================>
Scheduler is currently registering the learning rate.
Epoch 156, Pointwise Training loss 0.36785990432385474, Pointwise Validation loss 0.3225876788298289
Full training loss: 45.61462813615799, Full test loss: 3.8710521459579468
The current learning rate is: 1.220703125e-07
Epoch 157 started ======>
train batch for epoch #  157 ==============>
test batch for epoch #  157 ======================>
Scheduler is currently registering the learning rate.
Epoch 157, Pointwise Training loss 0.36761012264797766, Pointwise Validation loss 0.3225898742675781
Full training loss: 45.58365520834923, Full test loss: 3.8710784912109375
The current learning rate is: 1.220703125e-07
Epoch 158 started ======>
train batch for epoch #  158 ==============>
test batch for epoch #  158 ======================>
Scheduler is currently registering the learning rate.
Epoch 158, Pointwise Training loss 0.36767395826116683, Pointwise Validation loss 0.3225955267747243
Full training loss: 45.59157082438469, Full test loss: 3.871146321296692
The current learning rate is: 1.220703125e-07
Epoch 159 started ======>
train batch for epoch #  159 ==============>
test batch for epoch #  159 ======================>
Scheduler is currently registering the learning rate.
Epoch 159, Pointwise Training loss 0.36782700736676494, Pointwise Validation loss 0.32259583473205566
Full training loss: 45.61054891347885, Full test loss: 3.871150016784668
The current learning rate is: 1.220703125e-07
Epoch 160 started ======>
train batch for epoch #  160 ==============>
test batch for epoch #  160 ======================>
Scheduler is currently registering the learning rate.
Epoch 160, Pointwise Training loss 0.3679205222475913, Pointwise Validation loss 0.3226013531287511
Full training loss: 45.622144758701324, Full test loss: 3.8712162375450134
The current learning rate is: 1.220703125e-07
Epoch 161 started ======>
train batch for epoch #  161 ==============>
test batch for epoch #  161 ======================>
Scheduler is currently registering the learning rate.
Epoch 161, Pointwise Training loss 0.3677745932532895, Pointwise Validation loss 0.3225957577427228
Full training loss: 45.6040495634079, Full test loss: 3.871149092912674
The current learning rate is: 1.220703125e-07
Epoch 162 started ======>
train batch for epoch #  162 ==============>
test batch for epoch #  162 ======================>
Scheduler is currently registering the learning rate.
Epoch 162, Pointwise Training loss 0.3680103351512263, Pointwise Validation loss 0.32259076585372287
Full training loss: 45.63328155875206, Full test loss: 3.8710891902446747
The current learning rate is: 6.103515625e-08
Epoch 163 started ======>
train batch for epoch #  163 ==============>
test batch for epoch #  163 ======================>
Scheduler is currently registering the learning rate.
Epoch 163, Pointwise Training loss 0.3673949121467529, Pointwise Validation loss 0.32259252419074375
Full training loss: 45.55696910619736, Full test loss: 3.871110290288925
The current learning rate is: 6.103515625e-08
Epoch 164 started ======>
train batch for epoch #  164 ==============>
test batch for epoch #  164 ======================>
Scheduler is currently registering the learning rate.
Epoch 164, Pointwise Training loss 0.36796782766619035, Pointwise Validation loss 0.32259542991717655
Full training loss: 45.628010630607605, Full test loss: 3.8711451590061188
The current learning rate is: 6.103515625e-08
Epoch 165 started ======>
train batch for epoch #  165 ==============>
test batch for epoch #  165 ======================>
Scheduler is currently registering the learning rate.
Epoch 165, Pointwise Training loss 0.3674543795566405, Pointwise Validation loss 0.32259553919235867
Full training loss: 45.56434306502342, Full test loss: 3.871146470308304
The current learning rate is: 6.103515625e-08
Epoch 166 started ======>
train batch for epoch #  166 ==============>
test batch for epoch #  166 ======================>
Scheduler is currently registering the learning rate.
Epoch 166, Pointwise Training loss 0.3675086022384705, Pointwise Validation loss 0.32259154071410495
Full training loss: 45.57106667757034, Full test loss: 3.8710984885692596
The current learning rate is: 6.103515625e-08
Epoch 167 started ======>
train batch for epoch #  167 ==============>
test batch for epoch #  167 ======================>
Scheduler is currently registering the learning rate.
Epoch 167, Pointwise Training loss 0.36808644211099995, Pointwise Validation loss 0.3225913345813751
Full training loss: 45.64271882176399, Full test loss: 3.8710960149765015
The current learning rate is: 6.103515625e-08
Epoch 168 started ======>
train batch for epoch #  168 ==============>
test batch for epoch #  168 ======================>
Scheduler is currently registering the learning rate.
Epoch 168, Pointwise Training loss 0.36750901995166657, Pointwise Validation loss 0.3225930134455363
Full training loss: 45.57111847400665, Full test loss: 3.8711161613464355
The current learning rate is: 6.103515625e-08
Epoch 169 started ======>
train batch for epoch #  169 ==============>
test batch for epoch #  169 ======================>
Scheduler is currently registering the learning rate.
Epoch 169, Pointwise Training loss 0.3674250987748946, Pointwise Validation loss 0.32259464263916016
Full training loss: 45.56071224808693, Full test loss: 3.871135711669922
The current learning rate is: 6.103515625e-08
Epoch 170 started ======>
train batch for epoch #  170 ==============>
test batch for epoch #  170 ======================>
Scheduler is currently registering the learning rate.
Epoch 170, Pointwise Training loss 0.36769095832301724, Pointwise Validation loss 0.3225901896754901
Full training loss: 45.59367883205414, Full test loss: 3.8710822761058807
The current learning rate is: 6.103515625e-08
Epoch 171 started ======>
train batch for epoch #  171 ==============>
test batch for epoch #  171 ======================>
Scheduler is currently registering the learning rate.
Epoch 171, Pointwise Training loss 0.3679733740225915, Pointwise Validation loss 0.3225928172469139
Full training loss: 45.628698378801346, Full test loss: 3.871113806962967
The current learning rate is: 6.103515625e-08
Epoch 172 started ======>
train batch for epoch #  172 ==============>
test batch for epoch #  172 ======================>
Scheduler is currently registering the learning rate.
Epoch 172, Pointwise Training loss 0.36815990267261384, Pointwise Validation loss 0.32259934147198993
Full training loss: 45.651827931404114, Full test loss: 3.8711920976638794
The current learning rate is: 6.103515625e-08
Epoch 173 started ======>
train batch for epoch #  173 ==============>
test batch for epoch #  173 ======================>
Scheduler is currently registering the learning rate.
Epoch 173, Pointwise Training loss 0.36810251661846716, Pointwise Validation loss 0.3225955218076706
Full training loss: 45.644712060689926, Full test loss: 3.871146261692047
The current learning rate is: 3.0517578125e-08
Epoch 174 started ======>
train batch for epoch #  174 ==============>
test batch for epoch #  174 ======================>
Scheduler is currently registering the learning rate.
Epoch 174, Pointwise Training loss 0.36827731949667775, Pointwise Validation loss 0.322591337064902
Full training loss: 45.66638761758804, Full test loss: 3.871096044778824
The current learning rate is: 3.0517578125e-08
Epoch 175 started ======>
train batch for epoch #  175 ==============>
test batch for epoch #  175 ======================>
Scheduler is currently registering the learning rate.
Epoch 175, Pointwise Training loss 0.3677738977055396, Pointwise Validation loss 0.3225893552104632
Full training loss: 45.60396331548691, Full test loss: 3.8710722625255585
The current learning rate is: 3.0517578125e-08
Epoch 176 started ======>
train batch for epoch #  176 ==============>
test batch for epoch #  176 ======================>
Scheduler is currently registering the learning rate.
Epoch 176, Pointwise Training loss 0.3680158605979335, Pointwise Validation loss 0.32259292403856915
Full training loss: 45.63396671414375, Full test loss: 3.8711150884628296
The current learning rate is: 3.0517578125e-08
Epoch 177 started ======>
train batch for epoch #  177 ==============>
test batch for epoch #  177 ======================>
Scheduler is currently registering the learning rate.
Epoch 177, Pointwise Training loss 0.3678390520714944, Pointwise Validation loss 0.322595976293087
Full training loss: 45.61204245686531, Full test loss: 3.871151715517044
The current learning rate is: 3.0517578125e-08
Epoch 178 started ======>
train batch for epoch #  178 ==============>
test batch for epoch #  178 ======================>
Scheduler is currently registering the learning rate.
Epoch 178, Pointwise Training loss 0.3675819178742747, Pointwise Validation loss 0.3225903958082199
Full training loss: 45.580157816410065, Full test loss: 3.871084749698639
The current learning rate is: 3.0517578125e-08
Epoch 179 started ======>
train batch for epoch #  179 ==============>
test batch for epoch #  179 ======================>
Scheduler is currently registering the learning rate.
Epoch 179, Pointwise Training loss 0.3676957117934381, Pointwise Validation loss 0.32258841147025424
Full training loss: 45.59426826238632, Full test loss: 3.871060937643051
The current learning rate is: 3.0517578125e-08
Epoch 180 started ======>
train batch for epoch #  180 ==============>
test batch for epoch #  180 ======================>
Scheduler is currently registering the learning rate.
Epoch 180, Pointwise Training loss 0.36750701214036635, Pointwise Validation loss 0.3225921069582303
Full training loss: 45.570869505405426, Full test loss: 3.871105283498764
The current learning rate is: 3.0517578125e-08
Epoch 181 started ======>
train batch for epoch #  181 ==============>
test batch for epoch #  181 ======================>
Scheduler is currently registering the learning rate.
Epoch 181, Pointwise Training loss 0.36756480389064355, Pointwise Validation loss 0.32258959114551544
Full training loss: 45.578035682439804, Full test loss: 3.8710750937461853
The current learning rate is: 3.0517578125e-08
Epoch 182 started ======>
train batch for epoch #  182 ==============>
test batch for epoch #  182 ======================>
Scheduler is currently registering the learning rate.
Epoch 182, Pointwise Training loss 0.3680599307821643, Pointwise Validation loss 0.3225959663589795
Full training loss: 45.63943141698837, Full test loss: 3.8711515963077545
The current learning rate is: 3.0517578125e-08
Epoch 183 started ======>
train batch for epoch #  183 ==============>
test batch for epoch #  183 ======================>
Scheduler is currently registering the learning rate.
Epoch 183, Pointwise Training loss 0.3679841374677996, Pointwise Validation loss 0.32259069631497067
Full training loss: 45.630033046007156, Full test loss: 3.871088355779648
The current learning rate is: 3.0517578125e-08
Epoch 184 started ======>
train batch for epoch #  184 ==============>
test batch for epoch #  184 ======================>
Scheduler is currently registering the learning rate.
Epoch 184, Pointwise Training loss 0.3677816376570732, Pointwise Validation loss 0.32259228577216464
Full training loss: 45.60492306947708, Full test loss: 3.871107429265976
The current learning rate is: 1.52587890625e-08
Epoch 185 started ======>
train batch for epoch #  185 ==============>
test batch for epoch #  185 ======================>
Scheduler is currently registering the learning rate.
Epoch 185, Pointwise Training loss 0.3683616040695098, Pointwise Validation loss 0.32259270797173184
Full training loss: 45.67683890461922, Full test loss: 3.871112495660782
The current learning rate is: 1.52587890625e-08
Epoch 186 started ======>
train batch for epoch #  186 ==============>
test batch for epoch #  186 ======================>
Scheduler is currently registering the learning rate.
Epoch 186, Pointwise Training loss 0.36768479332808524, Pointwise Validation loss 0.32259153574705124
Full training loss: 45.59291437268257, Full test loss: 3.871098428964615
The current learning rate is: 1.52587890625e-08
Epoch 187 started ======>
train batch for epoch #  187 ==============>
test batch for epoch #  187 ======================>
Scheduler is currently registering the learning rate.
Epoch 187, Pointwise Training loss 0.3675688329723574, Pointwise Validation loss 0.3225905696551005
Full training loss: 45.57853528857231, Full test loss: 3.871086835861206
The current learning rate is: 1.52587890625e-08
Epoch 188 started ======>
train batch for epoch #  188 ==============>
test batch for epoch #  188 ======================>
Scheduler is currently registering the learning rate.
Epoch 188, Pointwise Training loss 0.3677719689665302, Pointwise Validation loss 0.3225902219613393
Full training loss: 45.60372415184975, Full test loss: 3.8710826635360718
The current learning rate is: 1.52587890625e-08
Epoch 189 started ======>
train batch for epoch #  189 ==============>
test batch for epoch #  189 ======================>
Scheduler is currently registering the learning rate.
Epoch 189, Pointwise Training loss 0.36825817775341774, Pointwise Validation loss 0.32258836179971695
Full training loss: 45.6640140414238, Full test loss: 3.8710603415966034
The current learning rate is: 1.52587890625e-08
Epoch 190 started ======>
train batch for epoch #  190 ==============>
test batch for epoch #  190 ======================>
Scheduler is currently registering the learning rate.
Epoch 190, Pointwise Training loss 0.3674497169352347, Pointwise Validation loss 0.3225978563229243
Full training loss: 45.5637648999691, Full test loss: 3.8711742758750916
The current learning rate is: 1.52587890625e-08
Epoch 191 started ======>
train batch for epoch #  191 ==============>
test batch for epoch #  191 ======================>
Scheduler is currently registering the learning rate.
Epoch 191, Pointwise Training loss 0.36822840595437634, Pointwise Validation loss 0.322588915626208
Full training loss: 45.66032233834267, Full test loss: 3.871066987514496
The current learning rate is: 1.52587890625e-08
Epoch 192 started ======>
train batch for epoch #  192 ==============>
test batch for epoch #  192 ======================>
Scheduler is currently registering the learning rate.
Epoch 192, Pointwise Training loss 0.36738543308550314, Pointwise Validation loss 0.32259060939153034
Full training loss: 45.55579370260239, Full test loss: 3.8710873126983643
The current learning rate is: 1.52587890625e-08
Epoch 193 started ======>
train batch for epoch #  193 ==============>
test batch for epoch #  193 ======================>
Scheduler is currently registering the learning rate.
Epoch 193, Pointwise Training loss 0.36809233479922815, Pointwise Validation loss 0.3225928197304408
Full training loss: 45.643449515104294, Full test loss: 3.8711138367652893
The current learning rate is: 1.52587890625e-08
Epoch 194 started ======>
train batch for epoch #  194 ==============>
test batch for epoch #  194 ======================>
Scheduler is currently registering the learning rate.
Epoch 194, Pointwise Training loss 0.3676575888549128, Pointwise Validation loss 0.32259395470221836
Full training loss: 45.589541018009186, Full test loss: 3.8711274564266205
The current learning rate is: 1.52587890625e-08
Epoch 195 started ======>
train batch for epoch #  195 ==============>
test batch for epoch #  195 ======================>
Scheduler is currently registering the learning rate.
Epoch 195, Pointwise Training loss 0.3679096270953455, Pointwise Validation loss 0.3226001386841138
Full training loss: 45.620793759822845, Full test loss: 3.871201664209366
The current learning rate is: 1.52587890625e-08
Epoch 196 started ======>
train batch for epoch #  196 ==============>
test batch for epoch #  196 ======================>
Scheduler is currently registering the learning rate.
Epoch 196, Pointwise Training loss 0.3675760559497341, Pointwise Validation loss 0.3225910887122154
Full training loss: 45.57943093776703, Full test loss: 3.871093064546585
The current learning rate is: 1.52587890625e-08
Epoch 197 started ======>
train batch for epoch #  197 ==============>
test batch for epoch #  197 ======================>
Scheduler is currently registering the learning rate.
Epoch 197, Pointwise Training loss 0.36814230872738746, Pointwise Validation loss 0.3225916251540184
Full training loss: 45.649646282196045, Full test loss: 3.871099501848221
The current learning rate is: 1.52587890625e-08
Epoch 198 started ======>
train batch for epoch #  198 ==============>
test batch for epoch #  198 ======================>
Scheduler is currently registering the learning rate.
Epoch 198, Pointwise Training loss 0.3676000869562549, Pointwise Validation loss 0.3225892633199692
Full training loss: 45.58241078257561, Full test loss: 3.87107115983963
The current learning rate is: 1.52587890625e-08
Epoch 199 started ======>
train batch for epoch #  199 ==============>
test batch for epoch #  199 ======================>
Scheduler is currently registering the learning rate.
Epoch 199, Pointwise Training loss 0.3676316163712932, Pointwise Validation loss 0.3225919306278229
Full training loss: 45.58632043004036, Full test loss: 3.8711031675338745
The current learning rate is: 1.52587890625e-08
Epoch 200 started ======>
train batch for epoch #  200 ==============>
test batch for epoch #  200 ======================>
Scheduler is currently registering the learning rate.
Epoch 200, Pointwise Training loss 0.36797980627705973, Pointwise Validation loss 0.32258910437424976
Full training loss: 45.62949597835541, Full test loss: 3.8710692524909973
The current learning rate is: 1.52587890625e-08
Epoch 201 started ======>
train batch for epoch #  201 ==============>
test batch for epoch #  201 ======================>
Scheduler is currently registering the learning rate.
Epoch 201, Pointwise Training loss 0.36782546725965315, Pointwise Validation loss 0.3225894942879677
Full training loss: 45.61035794019699, Full test loss: 3.871073931455612
The current learning rate is: 1.52587890625e-08
Epoch 202 started ======>
train batch for epoch #  202 ==============>
test batch for epoch #  202 ======================>
Scheduler is currently registering the learning rate.
Epoch 202, Pointwise Training loss 0.36759305192578223, Pointwise Validation loss 0.3225913619001706
Full training loss: 45.581538438797, Full test loss: 3.8710963428020477
The current learning rate is: 1.52587890625e-08
Epoch 203 started ======>
train batch for epoch #  203 ==============>
test batch for epoch #  203 ======================>
Scheduler is currently registering the learning rate.
Epoch 203, Pointwise Training loss 0.3679546444646774, Pointwise Validation loss 0.3225939944386482
Full training loss: 45.626375913619995, Full test loss: 3.8711279332637787
The current learning rate is: 1.52587890625e-08
Epoch 204 started ======>
train batch for epoch #  204 ==============>
test batch for epoch #  204 ======================>
Scheduler is currently registering the learning rate.
Epoch 204, Pointwise Training loss 0.3673986859859959, Pointwise Validation loss 0.322589931388696
Full training loss: 45.55743706226349, Full test loss: 3.8710791766643524
The current learning rate is: 1.52587890625e-08
Epoch 205 started ======>
train batch for epoch #  205 ==============>
test batch for epoch #  205 ======================>
Scheduler is currently registering the learning rate.
Epoch 205, Pointwise Training loss 0.3675256527719959, Pointwise Validation loss 0.3225897376736005
Full training loss: 45.57318094372749, Full test loss: 3.871076852083206
The current learning rate is: 1.52587890625e-08
Epoch 206 started ======>
train batch for epoch #  206 ==============>
test batch for epoch #  206 ======================>
Scheduler is currently registering the learning rate.
Epoch 206, Pointwise Training loss 0.3674402479683199, Pointwise Validation loss 0.32259559879700345
Full training loss: 45.56259074807167, Full test loss: 3.871147185564041
The current learning rate is: 1.52587890625e-08
Epoch 207 started ======>
train batch for epoch #  207 ==============>
test batch for epoch #  207 ======================>
Scheduler is currently registering the learning rate.
Epoch 207, Pointwise Training loss 0.3674205173888514, Pointwise Validation loss 0.32259294639031094
Full training loss: 45.560144156217575, Full test loss: 3.871115356683731
The current learning rate is: 1.52587890625e-08
Epoch 208 started ======>
train batch for epoch #  208 ==============>
test batch for epoch #  208 ======================>
Scheduler is currently registering the learning rate.
Epoch 208, Pointwise Training loss 0.3675551866331408, Pointwise Validation loss 0.32258888334035873
Full training loss: 45.57684314250946, Full test loss: 3.871066600084305
The current learning rate is: 1.52587890625e-08
Epoch 209 started ======>
train batch for epoch #  209 ==============>
test batch for epoch #  209 ======================>
Scheduler is currently registering the learning rate.
Epoch 209, Pointwise Training loss 0.36769036443964126, Pointwise Validation loss 0.3225897451241811
Full training loss: 45.59360519051552, Full test loss: 3.8710769414901733
The current learning rate is: 1.52587890625e-08
Epoch 210 started ======>
train batch for epoch #  210 ==============>
test batch for epoch #  210 ======================>
Scheduler is currently registering the learning rate.
Epoch 210, Pointwise Training loss 0.3676713217170008, Pointwise Validation loss 0.32259070376555127
Full training loss: 45.591243892908096, Full test loss: 3.871088445186615
The current learning rate is: 1.52587890625e-08
Epoch 211 started ======>
train batch for epoch #  211 ==============>
test batch for epoch #  211 ======================>
Scheduler is currently registering the learning rate.
Epoch 211, Pointwise Training loss 0.3680820722253092, Pointwise Validation loss 0.32259516169627506
Full training loss: 45.64217695593834, Full test loss: 3.871141940355301
The current learning rate is: 1.52587890625e-08
Epoch 212 started ======>
train batch for epoch #  212 ==============>
test batch for epoch #  212 ======================>
Scheduler is currently registering the learning rate.
Epoch 212, Pointwise Training loss 0.3678270753833555, Pointwise Validation loss 0.3225891689459483
Full training loss: 45.61055734753609, Full test loss: 3.8710700273513794
The current learning rate is: 1.52587890625e-08
Epoch 213 started ======>
train batch for epoch #  213 ==============>
test batch for epoch #  213 ======================>
Scheduler is currently registering the learning rate.
Epoch 213, Pointwise Training loss 0.3674605544055662, Pointwise Validation loss 0.32259538769721985
Full training loss: 45.56510874629021, Full test loss: 3.871144652366638
The current learning rate is: 1.52587890625e-08
Epoch 214 started ======>
train batch for epoch #  214 ==============>
test batch for epoch #  214 ======================>
Scheduler is currently registering the learning rate.
Epoch 214, Pointwise Training loss 0.36785862931320745, Pointwise Validation loss 0.3225949356953303
Full training loss: 45.61447003483772, Full test loss: 3.8711392283439636
The current learning rate is: 1.52587890625e-08
Epoch 215 started ======>
train batch for epoch #  215 ==============>
test batch for epoch #  215 ======================>
Scheduler is currently registering the learning rate.
Epoch 215, Pointwise Training loss 0.3675231710076332, Pointwise Validation loss 0.32259485125541687
Full training loss: 45.57287320494652, Full test loss: 3.8711382150650024
The current learning rate is: 1.52587890625e-08
Epoch 216 started ======>
train batch for epoch #  216 ==============>
test batch for epoch #  216 ======================>
Scheduler is currently registering the learning rate.
Epoch 216, Pointwise Training loss 0.36745021227867375, Pointwise Validation loss 0.3225888138016065
Full training loss: 45.56382632255554, Full test loss: 3.871065765619278
The current learning rate is: 1.52587890625e-08
Epoch 217 started ======>
train batch for epoch #  217 ==============>
test batch for epoch #  217 ======================>
Scheduler is currently registering the learning rate.
Epoch 217, Pointwise Training loss 0.36784939804384786, Pointwise Validation loss 0.3225960408647855
Full training loss: 45.613325357437134, Full test loss: 3.871152490377426
The current learning rate is: 1.52587890625e-08
Epoch 218 started ======>
train batch for epoch #  218 ==============>
test batch for epoch #  218 ======================>
Scheduler is currently registering the learning rate.
Epoch 218, Pointwise Training loss 0.3681395236522921, Pointwise Validation loss 0.32259464760621387
Full training loss: 45.649300932884216, Full test loss: 3.8711357712745667
The current learning rate is: 1.52587890625e-08
Epoch 219 started ======>
train batch for epoch #  219 ==============>
test batch for epoch #  219 ======================>
Scheduler is currently registering the learning rate.
Epoch 219, Pointwise Training loss 0.3676217698281811, Pointwise Validation loss 0.32259224355220795
Full training loss: 45.58509945869446, Full test loss: 3.8711069226264954
The current learning rate is: 1.52587890625e-08
Epoch 220 started ======>
train batch for epoch #  220 ==============>
test batch for epoch #  220 ======================>
Scheduler is currently registering the learning rate.
Epoch 220, Pointwise Training loss 0.3678301887646798, Pointwise Validation loss 0.3225895365079244
Full training loss: 45.6109434068203, Full test loss: 3.8710744380950928
The current learning rate is: 1.52587890625e-08
Epoch 221 started ======>
train batch for epoch #  221 ==============>
test batch for epoch #  221 ======================>
Scheduler is currently registering the learning rate.
Epoch 221, Pointwise Training loss 0.3678156118239126, Pointwise Validation loss 0.322589931388696
Full training loss: 45.60913586616516, Full test loss: 3.8710791766643524
The current learning rate is: 1.52587890625e-08
Epoch 222 started ======>
train batch for epoch #  222 ==============>
test batch for epoch #  222 ======================>
Scheduler is currently registering the learning rate.
Epoch 222, Pointwise Training loss 0.3674726608780123, Pointwise Validation loss 0.32258902738491696
Full training loss: 45.56660994887352, Full test loss: 3.8710683286190033
The current learning rate is: 1.52587890625e-08
Epoch 223 started ======>
train batch for epoch #  223 ==============>
test batch for epoch #  223 ======================>
Scheduler is currently registering the learning rate.
Epoch 223, Pointwise Training loss 0.3674778145167135, Pointwise Validation loss 0.32259046534697217
Full training loss: 45.56724900007248, Full test loss: 3.8710855841636658
The current learning rate is: 1.52587890625e-08
Epoch 224 started ======>
train batch for epoch #  224 ==============>
test batch for epoch #  224 ======================>
Scheduler is currently registering the learning rate.
Epoch 224, Pointwise Training loss 0.3676151356870128, Pointwise Validation loss 0.3225908875465393
Full training loss: 45.58427682518959, Full test loss: 3.8710906505584717
The current learning rate is: 1.52587890625e-08
Epoch 225 started ======>
train batch for epoch #  225 ==============>
test batch for epoch #  225 ======================>
Scheduler is currently registering the learning rate.
Epoch 225, Pointwise Training loss 0.3678270316412372, Pointwise Validation loss 0.3225901698072751
Full training loss: 45.61055192351341, Full test loss: 3.8710820376873016
The current learning rate is: 1.52587890625e-08
Epoch 226 started ======>
train batch for epoch #  226 ==============>
test batch for epoch #  226 ======================>
Scheduler is currently registering the learning rate.
Epoch 226, Pointwise Training loss 0.3677269859660056, Pointwise Validation loss 0.3225887070099513
Full training loss: 45.5981462597847, Full test loss: 3.8710644841194153
The current learning rate is: 1.52587890625e-08
Epoch 227 started ======>
train batch for epoch #  227 ==============>
test batch for epoch #  227 ======================>
Scheduler is currently registering the learning rate.
Epoch 227, Pointwise Training loss 0.36756248868280844, Pointwise Validation loss 0.3225871076186498
Full training loss: 45.57774859666824, Full test loss: 3.8710452914237976
The current learning rate is: 1.52587890625e-08
Epoch 228 started ======>
train batch for epoch #  228 ==============>
test batch for epoch #  228 ======================>
Scheduler is currently registering the learning rate.
Epoch 228, Pointwise Training loss 0.3674937818319567, Pointwise Validation loss 0.32258877406517666
Full training loss: 45.56922894716263, Full test loss: 3.8710652887821198
The current learning rate is: 1.52587890625e-08
Epoch 229 started ======>
train batch for epoch #  229 ==============>
test batch for epoch #  229 ======================>
Scheduler is currently registering the learning rate.
Epoch 229, Pointwise Training loss 0.36811476945877075, Pointwise Validation loss 0.3225876142581304
Full training loss: 45.64623141288757, Full test loss: 3.8710513710975647
The current learning rate is: 1.52587890625e-08
Epoch 230 started ======>
train batch for epoch #  230 ==============>
test batch for epoch #  230 ======================>
Scheduler is currently registering the learning rate.
Epoch 230, Pointwise Training loss 0.3678482424828314, Pointwise Validation loss 0.3225932642817497
Full training loss: 45.613182067871094, Full test loss: 3.8711191713809967
The current learning rate is: 1.52587890625e-08
Epoch 231 started ======>
train batch for epoch #  231 ==============>
test batch for epoch #  231 ======================>
Scheduler is currently registering the learning rate.
Epoch 231, Pointwise Training loss 0.36744588925953836, Pointwise Validation loss 0.32258940984805423
Full training loss: 45.563290268182755, Full test loss: 3.871072918176651
The current learning rate is: 1.52587890625e-08
Epoch 232 started ======>
train batch for epoch #  232 ==============>
test batch for epoch #  232 ======================>
Scheduler is currently registering the learning rate.
Epoch 232, Pointwise Training loss 0.36756196185465784, Pointwise Validation loss 0.32258959611256915
Full training loss: 45.57768326997757, Full test loss: 3.87107515335083
The current learning rate is: 1.52587890625e-08
Epoch 233 started ======>
train batch for epoch #  233 ==============>
test batch for epoch #  233 ======================>
Scheduler is currently registering the learning rate.
Epoch 233, Pointwise Training loss 0.3678382305848983, Pointwise Validation loss 0.3225889429450035
Full training loss: 45.61194059252739, Full test loss: 3.871067315340042
The current learning rate is: 1.52587890625e-08
Epoch 234 started ======>
train batch for epoch #  234 ==============>
test batch for epoch #  234 ======================>
Scheduler is currently registering the learning rate.
Epoch 234, Pointwise Training loss 0.36738196664279504, Pointwise Validation loss 0.32258691142002743
Full training loss: 45.55536386370659, Full test loss: 3.871042937040329
The current learning rate is: 1.52587890625e-08
Epoch 235 started ======>
train batch for epoch #  235 ==============>
test batch for epoch #  235 ======================>
Scheduler is currently registering the learning rate.
Epoch 235, Pointwise Training loss 0.3676968808135679, Pointwise Validation loss 0.32259129732847214
Full training loss: 45.594413220882416, Full test loss: 3.8710955679416656
The current learning rate is: 1.52587890625e-08
Epoch 236 started ======>
train batch for epoch #  236 ==============>
test batch for epoch #  236 ======================>
Scheduler is currently registering the learning rate.
Epoch 236, Pointwise Training loss 0.3675978914383919, Pointwise Validation loss 0.3225903883576393
Full training loss: 45.582138538360596, Full test loss: 3.8710846602916718
The current learning rate is: 1.52587890625e-08
Epoch 237 started ======>
train batch for epoch #  237 ==============>
test batch for epoch #  237 ======================>
Scheduler is currently registering the learning rate.
Epoch 237, Pointwise Training loss 0.3675376222498955, Pointwise Validation loss 0.3225894247492154
Full training loss: 45.574665158987045, Full test loss: 3.8710730969905853
The current learning rate is: 1.52587890625e-08
Epoch 238 started ======>
train batch for epoch #  238 ==============>
