[2024-06-10 17:03:24,626] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Read successful, here are the characteristics of your model: 
{'model_name': 'VideoMambaPose', 'project_dir': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/latent_space_regression_with_linear', 'model_type': 'latent_space_linear_regression', 'full_debug': False, 'show_gradients': False, 'show_predictions': False, 'embed_channels': 192, 'num_mamba_blocks': 12, 'num_deconv': 2, '2d_deconv': True, 'deconv_channels': 192, 'num_conv': 2, 'conv_channels': 256, 'joint_regressor': True, 'hidden_channels': 512, 'output_dimensions': 2, 'dropout': False, 'dropout_percent': 0.25, 'num_hidden_layers': 3, 'epoch_number': 300, 'batch_size': 4, 'learning_rate': 0.01, 'scheduler': True, 'start_epoch': 1, 'num_cpus': 1, 'num_gpus': 1, 'parallelize': False, 'checkpoint_directory': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/heatmap/checkpoint', 'checkpoint_name': 'cropped_tanh_overfit_model', 'follow_up': False, 'previous_training_epoch': 1, 'previous_checkpoint': '', 'dataset_name': 'JHMDB', 'data_path': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/data/JHMDB', 'annotations_path': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/data/JHMDB_old/annotations', 'use_videos': False, 'preprocess_videos': True, 'skip': ['wave'], 'num_frames': 16, 'channels': 3, 'image_height': 240, 'image_width': 320, 'image_tensor_height': 256, 'image_tensor_width': 192, 'patch_number': 192, 'patch_size': 16, 'jump': 1, 'joint_number': 15, 'real_job': False, 'normalized': True, 'default': False, 'min_norm': -1}
length of actions 1
The following are the actions:  ['shoot_bow']
The following are the train files:  [('shoot_bow', '11408ErikaRecurvefront_shoot_bow_u_nm_np1_fr_med_0', 40), ('shoot_bow', '11408ErikaRecurvefront_shoot_bow_u_nm_np1_fr_med_1', 40), ('shoot_bow', 'MegaPrecisionArchery_shoot_bow_u_nm_np1_fr_med_0', 40), ('shoot_bow', 'MegaPrecisionArchery_shoot_bow_u_nm_np1_fr_med_1', 40), ('shoot_bow', 'MegaPrecisionArchery_shoot_bow_u_nm_np1_fr_med_2', 40), ('shoot_bow', 'MegaPrecisionArchery_shoot_bow_u_nm_np1_fr_med_3', 35), ('shoot_bow', 'MegaPrecisionArchery_shoot_bow_u_nm_np1_fr_med_4', 40), ('shoot_bow', 'MegaPrecisionArchery_shoot_bow_u_nm_np1_fr_med_5', 40), ('shoot_bow', 'HannahFront22May08_shoot_bow_u_nm_np1_fr_med_0', 40), ('shoot_bow', 'HannahFront22May08_shoot_bow_u_nm_np1_fr_med_1', 40), ('shoot_bow', 'ArcherShootsFOBArrowat100Yards_shoot_bow_u_nm_np1_fr_med_0', 40), ('shoot_bow', 'ArcherShootsFOBArrowat100Yards_shoot_bow_u_nm_np1_fr_med_1', 40), ('shoot_bow', '11_4_08ErikaRecurveBack_shoot_bow_u_nm_np1_ba_med_0', 40), ('shoot_bow', '11_4_08ErikaRecurveBack_shoot_bow_u_nm_np1_ba_med_1', 40), ('shoot_bow', 'Updatedvideoofmeshootingteamusaarchery_shoot_bow_f_nm_np1_ri_med_1', 35), ('shoot_bow', 'Updatedvideoofmeshootingteamusaarchery_shoot_bow_f_nm_np1_ri_med_5', 40), ('shoot_bow', 'Updatedvideoofmeshootingteamusaarchery_shoot_bow_u_cm_np1_ba_med_6', 40), ('shoot_bow', 'Updatedvideoofmeshootingteamusaarchery_shoot_bow_u_cm_np1_ri_med_4', 40), ('shoot_bow', 'Updatedvideoofmeshootingteamusaarchery_shoot_bow_u_nm_np1_ba_med_2', 40), ('shoot_bow', 'Updatedvideoofmeshootingteamusaarchery_shoot_bow_u_nm_np1_ba_med_7', 40), ('shoot_bow', 'Updatedvideoofmeshootingteamusaarchery_shoot_bow_u_nm_np1_fr_med_3', 40)]
The following are the test files:  []
The length of actions, train and test are 1 ,  21 ,  0
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (35, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (35, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
bboxes.shape (40, 4)
length of actions 1
The following are the actions:  ['shoot_bow']
The following are the train files:  []
The following are the test files:  [('shoot_bow', '6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_0', 40), ('shoot_bow', '6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_1', 40)]
The length of actions, train and test are 1 ,  0 ,  2
bboxes.shape (40, 4)
bboxes.shape (40, 4)
num_workers is: 7, for 8 cores
Use checkpoint: False
Checkpoint number: 0
Model loaded successfully as follows:  HeatMapVideoMambaPose(
  (mamba): VisionMamba(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 192, kernel_size=(1, 16, 16), stride=(1, 16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (head_drop): Identity()
    (head): Linear(in_features=192, out_features=1000, bias=True)
    (drop_path): DropPath()
    (layers): ModuleList(
      (0-1): 2 x Block(
        (mixer): Mamba(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (act): SiLU()
          (x_proj): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj): Linear(in_features=12, out_features=384, bias=True)
          (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
        (norm): RMSNorm()
        (drop_path): Identity()
      )
      (2-11): 10 x Block(
        (mixer): Mamba(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (act): SiLU()
          (x_proj): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj): Linear(in_features=12, out_features=384, bias=True)
          (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
        (norm): RMSNorm()
        (drop_path): DropPath()
      )
    )
    (norm_f): RMSNorm()
  )
  (deconv): Deconv(
    (conv_layers): Sequential(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))
    )
    (deconv_layers): Sequential(
      (0): ConvTranspose2d(192, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (joints): JointOutput(
    (regressor): Sequential(
      (0): Linear(in_features=3072, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=2, bias=True)
      (3): Tanh()
    )
  )
)
The model has started training, with the following characteristics:
Epoch 1 started ======>
Memory before (in MB) 29.263872
The number of batches in the train_set is 124
The number of batches in the test_set is 12
train batch for epoch #  1 ==============>
The shape of the outputs is  torch.Size([4, 15, 2])
The shape of the labels are  torch.Size([4, 15, 2])
Memory after train_batch (in MB) 172.51328
test batch for epoch #  1 ======================>
Memory after test_batch (in MB) 190.864896
Scheduler is currently registering the learning rate.
Epoch 1, Pointwise Training loss 0.9868477573317866, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.36912190914154, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Best model saved at /home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/heatmap/checkpoint/cropped_tanh_overfit_model/heatmap_11.1142.pt
Model parameters are of the following size 231
Epoch 2 started ======>
train batch for epoch #  2 ==============>
test batch for epoch #  2 ======================>
Scheduler is currently registering the learning rate.
Epoch 2, Pointwise Training loss 0.9906976775776956, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.84651201963425, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Epoch 3 started ======>
train batch for epoch #  3 ==============>
test batch for epoch #  3 ======================>
Scheduler is currently registering the learning rate.
Epoch 3, Pointwise Training loss 0.9900329584075559, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76408684253693, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Epoch 4 started ======>
train batch for epoch #  4 ==============>
test batch for epoch #  4 ======================>
Scheduler is currently registering the learning rate.
Epoch 4, Pointwise Training loss 0.9914244475864595, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.93663150072098, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Epoch 5 started ======>
train batch for epoch #  5 ==============>
test batch for epoch #  5 ======================>
Scheduler is currently registering the learning rate.
Epoch 5, Pointwise Training loss 0.9902921035405128, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.79622083902359, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Epoch 6 started ======>
train batch for epoch #  6 ==============>
test batch for epoch #  6 ======================>
Scheduler is currently registering the learning rate.
Epoch 6, Pointwise Training loss 0.9899105102785172, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.74890327453613, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Epoch 7 started ======>
train batch for epoch #  7 ==============>
test batch for epoch #  7 ======================>
Scheduler is currently registering the learning rate.
Epoch 7, Pointwise Training loss 0.9918661204076582, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.99139893054962, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Epoch 8 started ======>
train batch for epoch #  8 ==============>
test batch for epoch #  8 ======================>
Scheduler is currently registering the learning rate.
Epoch 8, Pointwise Training loss 0.9909368681330835, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.87617164850235, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Epoch 9 started ======>
train batch for epoch #  9 ==============>
test batch for epoch #  9 ======================>
Scheduler is currently registering the learning rate.
Epoch 9, Pointwise Training loss 0.9902812973145516, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.7948808670044, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Epoch 10 started ======>
train batch for epoch #  10 ==============>
test batch for epoch #  10 ======================>
Scheduler is currently registering the learning rate.
Epoch 10, Pointwise Training loss 0.9920724894731275, Pointwise Validation loss 0.9261795034011205
Full training loss: 123.01698869466782, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Epoch 11 started ======>
train batch for epoch #  11 ==============>
test batch for epoch #  11 ======================>
Scheduler is currently registering the learning rate.
Epoch 11, Pointwise Training loss 0.9919268815748153, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.9989333152771, Full test loss: 11.114154040813446
The current learning rate is: 0.01
Epoch 12 started ======>
train batch for epoch #  12 ==============>
test batch for epoch #  12 ======================>
Scheduler is currently registering the learning rate.
Epoch 12, Pointwise Training loss 0.9900568891917506, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76705425977707, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 13 started ======>
train batch for epoch #  13 ==============>
test batch for epoch #  13 ======================>
Scheduler is currently registering the learning rate.
Epoch 13, Pointwise Training loss 0.9909539246751417, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.87828665971756, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 14 started ======>
train batch for epoch #  14 ==============>
test batch for epoch #  14 ======================>
Scheduler is currently registering the learning rate.
Epoch 14, Pointwise Training loss 0.9900609663417262, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76755982637405, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 15 started ======>
train batch for epoch #  15 ==============>
test batch for epoch #  15 ======================>
Scheduler is currently registering the learning rate.
Epoch 15, Pointwise Training loss 0.9911440086941565, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.90185707807541, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 16 started ======>
train batch for epoch #  16 ==============>
test batch for epoch #  16 ======================>
Scheduler is currently registering the learning rate.
Epoch 16, Pointwise Training loss 0.9901332364928338, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.77652132511139, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 17 started ======>
train batch for epoch #  17 ==============>
test batch for epoch #  17 ======================>
Scheduler is currently registering the learning rate.
Epoch 17, Pointwise Training loss 0.9902259231575073, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78801447153091, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 18 started ======>
train batch for epoch #  18 ==============>
test batch for epoch #  18 ======================>
Scheduler is currently registering the learning rate.
Epoch 18, Pointwise Training loss 0.9902085238887418, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78585696220398, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 19 started ======>
train batch for epoch #  19 ==============>
test batch for epoch #  19 ======================>
Scheduler is currently registering the learning rate.
Epoch 19, Pointwise Training loss 0.9913359491094467, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.92565768957138, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 20 started ======>
train batch for epoch #  20 ==============>
test batch for epoch #  20 ======================>
Scheduler is currently registering the learning rate.
Epoch 20, Pointwise Training loss 0.990016758441925, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.7620780467987, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 21 started ======>
train batch for epoch #  21 ==============>
test batch for epoch #  21 ======================>
Scheduler is currently registering the learning rate.
Epoch 21, Pointwise Training loss 0.9913914107507275, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.93253493309021, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 22 started ======>
train batch for epoch #  22 ==============>
test batch for epoch #  22 ======================>
Scheduler is currently registering the learning rate.
Epoch 22, Pointwise Training loss 0.9909408433783439, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.87666457891464, Full test loss: 11.114154040813446
The current learning rate is: 0.005
Epoch 23 started ======>
train batch for epoch #  23 ==============>
test batch for epoch #  23 ======================>
Scheduler is currently registering the learning rate.
Epoch 23, Pointwise Training loss 0.9912761791098502, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.91824620962143, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 24 started ======>
train batch for epoch #  24 ==============>
test batch for epoch #  24 ======================>
Scheduler is currently registering the learning rate.
Epoch 24, Pointwise Training loss 0.9902378647558151, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78949522972107, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 25 started ======>
train batch for epoch #  25 ==============>
test batch for epoch #  25 ======================>
Scheduler is currently registering the learning rate.
Epoch 25, Pointwise Training loss 0.9903769055681844, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.80673629045486, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 26 started ======>
train batch for epoch #  26 ==============>
test batch for epoch #  26 ======================>
Scheduler is currently registering the learning rate.
Epoch 26, Pointwise Training loss 0.9912486124423242, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.9148279428482, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 27 started ======>
train batch for epoch #  27 ==============>
test batch for epoch #  27 ======================>
Scheduler is currently registering the learning rate.
Epoch 27, Pointwise Training loss 0.9907822229208485, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.85699564218521, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 28 started ======>
train batch for epoch #  28 ==============>
test batch for epoch #  28 ======================>
Scheduler is currently registering the learning rate.
Epoch 28, Pointwise Training loss 0.990941206774404, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.87670964002609, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 29 started ======>
train batch for epoch #  29 ==============>
test batch for epoch #  29 ======================>
Scheduler is currently registering the learning rate.
Epoch 29, Pointwise Training loss 0.9900370288279748, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76459157466888, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 30 started ======>
train batch for epoch #  30 ==============>
test batch for epoch #  30 ======================>
Scheduler is currently registering the learning rate.
Epoch 30, Pointwise Training loss 0.9900572170172969, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.7670949101448, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 31 started ======>
train batch for epoch #  31 ==============>
test batch for epoch #  31 ======================>
Scheduler is currently registering the learning rate.
Epoch 31, Pointwise Training loss 0.99003792722379, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76470297574997, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 32 started ======>
train batch for epoch #  32 ==============>
test batch for epoch #  32 ======================>
Scheduler is currently registering the learning rate.
Epoch 32, Pointwise Training loss 0.9900103990108736, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76128947734833, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 33 started ======>
train batch for epoch #  33 ==============>
test batch for epoch #  33 ======================>
Scheduler is currently registering the learning rate.
Epoch 33, Pointwise Training loss 0.9899779135181058, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.75726127624512, Full test loss: 11.114154040813446
The current learning rate is: 0.0025
Epoch 34 started ======>
train batch for epoch #  34 ==============>
test batch for epoch #  34 ======================>
Scheduler is currently registering the learning rate.
Epoch 34, Pointwise Training loss 0.9909048950479876, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.87220698595047, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 35 started ======>
train batch for epoch #  35 ==============>
test batch for epoch #  35 ======================>
Scheduler is currently registering the learning rate.
Epoch 35, Pointwise Training loss 0.9912021780206312, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.90907007455826, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 36 started ======>
train batch for epoch #  36 ==============>
test batch for epoch #  36 ======================>
Scheduler is currently registering the learning rate.
Epoch 36, Pointwise Training loss 0.9901875718947379, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78325891494751, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 37 started ======>
train batch for epoch #  37 ==============>
test batch for epoch #  37 ======================>
Scheduler is currently registering the learning rate.
Epoch 37, Pointwise Training loss 0.9900582427940061, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76722210645676, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 38 started ======>
train batch for epoch #  38 ==============>
test batch for epoch #  38 ======================>
Scheduler is currently registering the learning rate.
Epoch 38, Pointwise Training loss 0.9911123052720101, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.89792585372925, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 39 started ======>
train batch for epoch #  39 ==============>
test batch for epoch #  39 ======================>
Scheduler is currently registering the learning rate.
Epoch 39, Pointwise Training loss 0.9919545674516309, Pointwise Validation loss 0.9261795034011205
Full training loss: 123.00236636400223, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 40 started ======>
train batch for epoch #  40 ==============>
test batch for epoch #  40 ======================>
Scheduler is currently registering the learning rate.
Epoch 40, Pointwise Training loss 0.9899141629857402, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.74935621023178, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 41 started ======>
train batch for epoch #  41 ==============>
test batch for epoch #  41 ======================>
Scheduler is currently registering the learning rate.
Epoch 41, Pointwise Training loss 0.9912912470679129, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.9201146364212, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 42 started ======>
train batch for epoch #  42 ==============>
test batch for epoch #  42 ======================>
Scheduler is currently registering the learning rate.
Epoch 42, Pointwise Training loss 0.990167417833882, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78075981140137, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 43 started ======>
train batch for epoch #  43 ==============>
test batch for epoch #  43 ======================>
Scheduler is currently registering the learning rate.
Epoch 43, Pointwise Training loss 0.990866647612664, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.86746430397034, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 44 started ======>
train batch for epoch #  44 ==============>
test batch for epoch #  44 ======================>
Scheduler is currently registering the learning rate.
Epoch 44, Pointwise Training loss 0.9902800932045905, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.79473155736923, Full test loss: 11.114154040813446
The current learning rate is: 0.00125
Epoch 45 started ======>
train batch for epoch #  45 ==============>
test batch for epoch #  45 ======================>
Scheduler is currently registering the learning rate.
Epoch 45, Pointwise Training loss 0.990266195228023, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.79300820827484, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 46 started ======>
train batch for epoch #  46 ==============>
test batch for epoch #  46 ======================>
Scheduler is currently registering the learning rate.
Epoch 46, Pointwise Training loss 0.9900757064742427, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76938760280609, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 47 started ======>
train batch for epoch #  47 ==============>
test batch for epoch #  47 ======================>
Scheduler is currently registering the learning rate.
Epoch 47, Pointwise Training loss 0.990054843887206, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76680064201355, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 48 started ======>
train batch for epoch #  48 ==============>
test batch for epoch #  48 ======================>
Scheduler is currently registering the learning rate.
Epoch 48, Pointwise Training loss 0.9901890024062125, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78343629837036, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 49 started ======>
train batch for epoch #  49 ==============>
test batch for epoch #  49 ======================>
Scheduler is currently registering the learning rate.
Epoch 49, Pointwise Training loss 0.9922148287296295, Pointwise Validation loss 0.9261795034011205
Full training loss: 123.03463876247406, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 50 started ======>
train batch for epoch #  50 ==============>
test batch for epoch #  50 ======================>
Scheduler is currently registering the learning rate.
Epoch 50, Pointwise Training loss 0.9902441790027003, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.79027819633484, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 51 started ======>
train batch for epoch #  51 ==============>
test batch for epoch #  51 ======================>
Scheduler is currently registering the learning rate.
Epoch 51, Pointwise Training loss 0.9900558533207062, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76692581176758, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 52 started ======>
train batch for epoch #  52 ==============>
test batch for epoch #  52 ======================>
Scheduler is currently registering the learning rate.
Epoch 52, Pointwise Training loss 0.9899501800537109, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.75382232666016, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 53 started ======>
train batch for epoch #  53 ==============>
test batch for epoch #  53 ======================>
Scheduler is currently registering the learning rate.
Epoch 53, Pointwise Training loss 0.9901219414126489, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.77512073516846, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 54 started ======>
train batch for epoch #  54 ==============>
test batch for epoch #  54 ======================>
Scheduler is currently registering the learning rate.
Epoch 54, Pointwise Training loss 0.9905634176346564, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.82986378669739, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 55 started ======>
train batch for epoch #  55 ==============>
test batch for epoch #  55 ======================>
Scheduler is currently registering the learning rate.
Epoch 55, Pointwise Training loss 0.9901608161387905, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.77994120121002, Full test loss: 11.114154040813446
The current learning rate is: 0.000625
Epoch 56 started ======>
train batch for epoch #  56 ==============>
test batch for epoch #  56 ======================>
Scheduler is currently registering the learning rate.
Epoch 56, Pointwise Training loss 0.9901730100954732, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78145325183868, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 57 started ======>
train batch for epoch #  57 ==============>
test batch for epoch #  57 ======================>
Scheduler is currently registering the learning rate.
Epoch 57, Pointwise Training loss 0.9909762456532447, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.88105446100235, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 58 started ======>
train batch for epoch #  58 ==============>
test batch for epoch #  58 ======================>
Scheduler is currently registering the learning rate.
Epoch 58, Pointwise Training loss 0.9920402952740269, Pointwise Validation loss 0.9261795034011205
Full training loss: 123.01299661397934, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 59 started ======>
train batch for epoch #  59 ==============>
test batch for epoch #  59 ======================>
Scheduler is currently registering the learning rate.
Epoch 59, Pointwise Training loss 0.9902385165614467, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78957605361938, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 60 started ======>
train batch for epoch #  60 ==============>
test batch for epoch #  60 ======================>
Scheduler is currently registering the learning rate.
Epoch 60, Pointwise Training loss 0.9900420125453703, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76520955562592, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 61 started ======>
train batch for epoch #  61 ==============>
test batch for epoch #  61 ======================>
Scheduler is currently registering the learning rate.
Epoch 61, Pointwise Training loss 0.9901401299622751, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.77737611532211, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 62 started ======>
train batch for epoch #  62 ==============>
test batch for epoch #  62 ======================>
Scheduler is currently registering the learning rate.
Epoch 62, Pointwise Training loss 0.9902111263044419, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.7861796617508, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 63 started ======>
train batch for epoch #  63 ==============>
test batch for epoch #  63 ======================>
Scheduler is currently registering the learning rate.
Epoch 63, Pointwise Training loss 0.9899762085368556, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.7570498585701, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 64 started ======>
train batch for epoch #  64 ==============>
test batch for epoch #  64 ======================>
Scheduler is currently registering the learning rate.
Epoch 64, Pointwise Training loss 0.990434264463763, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.81384879350662, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 65 started ======>
train batch for epoch #  65 ==============>
test batch for epoch #  65 ======================>
Scheduler is currently registering the learning rate.
Epoch 65, Pointwise Training loss 0.9900416491493103, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76516449451447, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 66 started ======>
train batch for epoch #  66 ==============>
test batch for epoch #  66 ======================>
Scheduler is currently registering the learning rate.
Epoch 66, Pointwise Training loss 0.9900480907770896, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.7659632563591, Full test loss: 11.114154040813446
The current learning rate is: 0.0003125
Epoch 67 started ======>
train batch for epoch #  67 ==============>
test batch for epoch #  67 ======================>
Scheduler is currently registering the learning rate.
Epoch 67, Pointwise Training loss 0.9910271691699182, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.88736897706985, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 68 started ======>
train batch for epoch #  68 ==============>
test batch for epoch #  68 ======================>
Scheduler is currently registering the learning rate.
Epoch 68, Pointwise Training loss 0.9904563311607607, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.81658506393433, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 69 started ======>
train batch for epoch #  69 ==============>
test batch for epoch #  69 ======================>
Scheduler is currently registering the learning rate.
Epoch 69, Pointwise Training loss 0.9901756278930172, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78177785873413, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 70 started ======>
train batch for epoch #  70 ==============>
test batch for epoch #  70 ======================>
Scheduler is currently registering the learning rate.
Epoch 70, Pointwise Training loss 0.9903027573900838, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.79754191637039, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 71 started ======>
train batch for epoch #  71 ==============>
test batch for epoch #  71 ======================>
Scheduler is currently registering the learning rate.
Epoch 71, Pointwise Training loss 0.991181805729866, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.90654391050339, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 72 started ======>
train batch for epoch #  72 ==============>
test batch for epoch #  72 ======================>
Scheduler is currently registering the learning rate.
Epoch 72, Pointwise Training loss 0.9908494314839763, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.86532950401306, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 73 started ======>
train batch for epoch #  73 ==============>
test batch for epoch #  73 ======================>
Scheduler is currently registering the learning rate.
Epoch 73, Pointwise Training loss 0.9902651276319258, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.7928758263588, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 74 started ======>
train batch for epoch #  74 ==============>
test batch for epoch #  74 ======================>
Scheduler is currently registering the learning rate.
Epoch 74, Pointwise Training loss 0.9909613324749854, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.8792052268982, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 75 started ======>
train batch for epoch #  75 ==============>
test batch for epoch #  75 ======================>
Scheduler is currently registering the learning rate.
Epoch 75, Pointwise Training loss 0.9916191524074923, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.96077489852905, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 76 started ======>
train batch for epoch #  76 ==============>
test batch for epoch #  76 ======================>
Scheduler is currently registering the learning rate.
Epoch 76, Pointwise Training loss 0.9899835081831101, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.75795501470566, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 77 started ======>
train batch for epoch #  77 ==============>
test batch for epoch #  77 ======================>
Scheduler is currently registering the learning rate.
Epoch 77, Pointwise Training loss 0.991270495999244, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.91754150390625, Full test loss: 11.114154040813446
The current learning rate is: 0.00015625
Epoch 78 started ======>
train batch for epoch #  78 ==============>
test batch for epoch #  78 ======================>
Scheduler is currently registering the learning rate.
Epoch 78, Pointwise Training loss 0.9901422166055248, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.77763485908508, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 79 started ======>
train batch for epoch #  79 ==============>
test batch for epoch #  79 ======================>
Scheduler is currently registering the learning rate.
Epoch 79, Pointwise Training loss 0.9903718415767916, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.80610835552216, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 80 started ======>
train batch for epoch #  80 ==============>
test batch for epoch #  80 ======================>
Scheduler is currently registering the learning rate.
Epoch 80, Pointwise Training loss 0.9901724750957182, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78138691186905, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 81 started ======>
train batch for epoch #  81 ==============>
test batch for epoch #  81 ======================>
Scheduler is currently registering the learning rate.
Epoch 81, Pointwise Training loss 0.9911765374483601, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.90589064359665, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 82 started ======>
train batch for epoch #  82 ==============>
test batch for epoch #  82 ======================>
Scheduler is currently registering the learning rate.
Epoch 82, Pointwise Training loss 0.9908722903459303, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.86816400289536, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 83 started ======>
train batch for epoch #  83 ==============>
test batch for epoch #  83 ======================>
Scheduler is currently registering the learning rate.
Epoch 83, Pointwise Training loss 0.9900616162246273, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76764041185379, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 84 started ======>
train batch for epoch #  84 ==============>
test batch for epoch #  84 ======================>
Scheduler is currently registering the learning rate.
Epoch 84, Pointwise Training loss 0.9909655682502254, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.87973046302795, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 85 started ======>
train batch for epoch #  85 ==============>
test batch for epoch #  85 ======================>
Scheduler is currently registering the learning rate.
Epoch 85, Pointwise Training loss 0.9900366476466579, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76454430818558, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 86 started ======>
train batch for epoch #  86 ==============>
test batch for epoch #  86 ======================>
Scheduler is currently registering the learning rate.
Epoch 86, Pointwise Training loss 0.9909704572731449, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.88033670186996, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 87 started ======>
train batch for epoch #  87 ==============>
test batch for epoch #  87 ======================>
Scheduler is currently registering the learning rate.
Epoch 87, Pointwise Training loss 0.9907863010321895, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.85750132799149, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 88 started ======>
train batch for epoch #  88 ==============>
test batch for epoch #  88 ======================>
Scheduler is currently registering the learning rate.
Epoch 88, Pointwise Training loss 0.9901946470622094, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78413623571396, Full test loss: 11.114154040813446
The current learning rate is: 7.8125e-05
Epoch 89 started ======>
train batch for epoch #  89 ==============>
test batch for epoch #  89 ======================>
Scheduler is currently registering the learning rate.
Epoch 89, Pointwise Training loss 0.9900245666503906, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76304626464844, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 90 started ======>
train batch for epoch #  90 ==============>
test batch for epoch #  90 ======================>
Scheduler is currently registering the learning rate.
Epoch 90, Pointwise Training loss 0.990980907793968, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.88163256645203, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 91 started ======>
train batch for epoch #  91 ==============>
test batch for epoch #  91 ======================>
Scheduler is currently registering the learning rate.
Epoch 91, Pointwise Training loss 0.9904863392153094, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.82030606269836, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 92 started ======>
train batch for epoch #  92 ==============>
test batch for epoch #  92 ======================>
Scheduler is currently registering the learning rate.
Epoch 92, Pointwise Training loss 0.9911218194230911, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.89910560846329, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 93 started ======>
train batch for epoch #  93 ==============>
test batch for epoch #  93 ======================>
Scheduler is currently registering the learning rate.
Epoch 93, Pointwise Training loss 0.9899164572838814, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.7496407032013, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 94 started ======>
train batch for epoch #  94 ==============>
test batch for epoch #  94 ======================>
Scheduler is currently registering the learning rate.
Epoch 94, Pointwise Training loss 0.9916962495734615, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.97033494710922, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 95 started ======>
train batch for epoch #  95 ==============>
test batch for epoch #  95 ======================>
Scheduler is currently registering the learning rate.
Epoch 95, Pointwise Training loss 0.9899157987486932, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.74955904483795, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 96 started ======>
train batch for epoch #  96 ==============>
test batch for epoch #  96 ======================>
Scheduler is currently registering the learning rate.
Epoch 96, Pointwise Training loss 0.9912322783662427, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.9128025174141, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 97 started ======>
train batch for epoch #  97 ==============>
test batch for epoch #  97 ======================>
Scheduler is currently registering the learning rate.
Epoch 97, Pointwise Training loss 0.9900124640234055, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76154553890228, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 98 started ======>
train batch for epoch #  98 ==============>
test batch for epoch #  98 ======================>
Scheduler is currently registering the learning rate.
Epoch 98, Pointwise Training loss 0.9900402253673922, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76498794555664, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 99 started ======>
train batch for epoch #  99 ==============>
test batch for epoch #  99 ======================>
Scheduler is currently registering the learning rate.
Epoch 99, Pointwise Training loss 0.9917217329625161, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.97349488735199, Full test loss: 11.114154040813446
The current learning rate is: 3.90625e-05
Epoch 100 started ======>
train batch for epoch #  100 ==============>
test batch for epoch #  100 ======================>
Scheduler is currently registering the learning rate.
Epoch 100, Pointwise Training loss 0.9903807654496162, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.80721491575241, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 101 started ======>
train batch for epoch #  101 ==============>
test batch for epoch #  101 ======================>
Scheduler is currently registering the learning rate.
Epoch 101, Pointwise Training loss 0.990034008218396, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76421701908112, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 102 started ======>
train batch for epoch #  102 ==============>
test batch for epoch #  102 ======================>
Scheduler is currently registering the learning rate.
Epoch 102, Pointwise Training loss 0.9909389360297111, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.87642806768417, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 103 started ======>
train batch for epoch #  103 ==============>
test batch for epoch #  103 ======================>
Scheduler is currently registering the learning rate.
Epoch 103, Pointwise Training loss 0.9907976355283491, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.85890680551529, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 104 started ======>
train batch for epoch #  104 ==============>
test batch for epoch #  104 ======================>
Scheduler is currently registering the learning rate.
Epoch 104, Pointwise Training loss 0.9909644001914609, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.87958562374115, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 105 started ======>
train batch for epoch #  105 ==============>
test batch for epoch #  105 ======================>
Scheduler is currently registering the learning rate.
Epoch 105, Pointwise Training loss 0.9902297243956597, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.7884858250618, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 106 started ======>
train batch for epoch #  106 ==============>
test batch for epoch #  106 ======================>
Scheduler is currently registering the learning rate.
Epoch 106, Pointwise Training loss 0.9912425755493103, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.91407936811447, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 107 started ======>
train batch for epoch #  107 ==============>
test batch for epoch #  107 ======================>
Scheduler is currently registering the learning rate.
Epoch 107, Pointwise Training loss 0.9912287145853043, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.91236060857773, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 108 started ======>
train batch for epoch #  108 ==============>
test batch for epoch #  108 ======================>
Scheduler is currently registering the learning rate.
Epoch 108, Pointwise Training loss 0.9907195928596682, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.84922951459885, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 109 started ======>
train batch for epoch #  109 ==============>
test batch for epoch #  109 ======================>
Scheduler is currently registering the learning rate.
Epoch 109, Pointwise Training loss 0.9910794169672074, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.89384770393372, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 110 started ======>
train batch for epoch #  110 ==============>
test batch for epoch #  110 ======================>
Scheduler is currently registering the learning rate.
Epoch 110, Pointwise Training loss 0.9914863273020713, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.94430458545685, Full test loss: 11.114154040813446
The current learning rate is: 1.953125e-05
Epoch 111 started ======>
train batch for epoch #  111 ==============>
test batch for epoch #  111 ======================>
Scheduler is currently registering the learning rate.
Epoch 111, Pointwise Training loss 0.9912472718184994, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.91466170549393, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 112 started ======>
train batch for epoch #  112 ==============>
test batch for epoch #  112 ======================>
Scheduler is currently registering the learning rate.
Epoch 112, Pointwise Training loss 0.9913580677201671, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.92840039730072, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 113 started ======>
train batch for epoch #  113 ==============>
test batch for epoch #  113 ======================>
Scheduler is currently registering the learning rate.
Epoch 113, Pointwise Training loss 0.9909572880114278, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.87870371341705, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 114 started ======>
train batch for epoch #  114 ==============>
test batch for epoch #  114 ======================>
Scheduler is currently registering the learning rate.
Epoch 114, Pointwise Training loss 0.9912410618797425, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.91389167308807, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 115 started ======>
train batch for epoch #  115 ==============>
test batch for epoch #  115 ======================>
Scheduler is currently registering the learning rate.
Epoch 115, Pointwise Training loss 0.9900275089087025, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76341110467911, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 116 started ======>
train batch for epoch #  116 ==============>
test batch for epoch #  116 ======================>
Scheduler is currently registering the learning rate.
Epoch 116, Pointwise Training loss 0.9913285941846909, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.92474567890167, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 117 started ======>
train batch for epoch #  117 ==============>
test batch for epoch #  117 ======================>
Scheduler is currently registering the learning rate.
Epoch 117, Pointwise Training loss 0.9899471959760112, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.75345230102539, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 118 started ======>
train batch for epoch #  118 ==============>
test batch for epoch #  118 ======================>
Scheduler is currently registering the learning rate.
Epoch 118, Pointwise Training loss 0.9900096986562975, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.76120263338089, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 119 started ======>
train batch for epoch #  119 ==============>
test batch for epoch #  119 ======================>
Scheduler is currently registering the learning rate.
Epoch 119, Pointwise Training loss 0.9917455748204262, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.97645127773285, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 120 started ======>
train batch for epoch #  120 ==============>
test batch for epoch #  120 ======================>
Scheduler is currently registering the learning rate.
Epoch 120, Pointwise Training loss 0.9902986672616774, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.797034740448, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 121 started ======>
train batch for epoch #  121 ==============>
test batch for epoch #  121 ======================>
Scheduler is currently registering the learning rate.
Epoch 121, Pointwise Training loss 0.9900842650282767, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.77044886350632, Full test loss: 11.114154040813446
The current learning rate is: 9.765625e-06
Epoch 122 started ======>
train batch for epoch #  122 ==============>
test batch for epoch #  122 ======================>
Scheduler is currently registering the learning rate.
Epoch 122, Pointwise Training loss 0.9901051819324493, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.77304255962372, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 123 started ======>
train batch for epoch #  123 ==============>
test batch for epoch #  123 ======================>
Scheduler is currently registering the learning rate.
Epoch 123, Pointwise Training loss 0.9901096493967118, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.77359652519226, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 124 started ======>
train batch for epoch #  124 ==============>
test batch for epoch #  124 ======================>
Scheduler is currently registering the learning rate.
Epoch 124, Pointwise Training loss 0.9922879540151165, Pointwise Validation loss 0.9261795034011205
Full training loss: 123.04370629787445, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 125 started ======>
train batch for epoch #  125 ==============>
test batch for epoch #  125 ======================>
Scheduler is currently registering the learning rate.
Epoch 125, Pointwise Training loss 0.9902281088213767, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78828549385071, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 126 started ======>
train batch for epoch #  126 ==============>
test batch for epoch #  126 ======================>
Scheduler is currently registering the learning rate.
Epoch 126, Pointwise Training loss 0.9902408074948096, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.78986012935638, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 127 started ======>
train batch for epoch #  127 ==============>
test batch for epoch #  127 ======================>
Scheduler is currently registering the learning rate.
Epoch 127, Pointwise Training loss 0.9911554595155101, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.90327697992325, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 128 started ======>
train batch for epoch #  128 ==============>
test batch for epoch #  128 ======================>
Scheduler is currently registering the learning rate.
Epoch 128, Pointwise Training loss 0.9912285232736219, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.91233688592911, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 129 started ======>
train batch for epoch #  129 ==============>
test batch for epoch #  129 ======================>
Scheduler is currently registering the learning rate.
Epoch 129, Pointwise Training loss 0.9902520569101456, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.79125505685806, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 130 started ======>
train batch for epoch #  130 ==============>
test batch for epoch #  130 ======================>
Scheduler is currently registering the learning rate.
Epoch 130, Pointwise Training loss 0.9902445928704354, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.79032951593399, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 131 started ======>
train batch for epoch #  131 ==============>
test batch for epoch #  131 ======================>
Scheduler is currently registering the learning rate.
Epoch 131, Pointwise Training loss 0.9900886940379297, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.77099806070328, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 132 started ======>
train batch for epoch #  132 ==============>
test batch for epoch #  132 ======================>
Scheduler is currently registering the learning rate.
Epoch 132, Pointwise Training loss 0.9913509233343986, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.92751449346542, Full test loss: 11.114154040813446
The current learning rate is: 4.8828125e-06
Epoch 133 started ======>
train batch for epoch #  133 ==============>
test batch for epoch #  133 ======================>
Scheduler is currently registering the learning rate.
Epoch 133, Pointwise Training loss 0.9912868944867965, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.91957491636276, Full test loss: 11.114154040813446
The current learning rate is: 2.44140625e-06
Epoch 134 started ======>
train batch for epoch #  134 ==============>
test batch for epoch #  134 ======================>
Scheduler is currently registering the learning rate.
Epoch 134, Pointwise Training loss 0.9920319184180229, Pointwise Validation loss 0.9261795034011205
Full training loss: 123.01195788383484, Full test loss: 11.114154040813446
The current learning rate is: 2.44140625e-06
Epoch 135 started ======>
train batch for epoch #  135 ==============>
test batch for epoch #  135 ======================>
Scheduler is currently registering the learning rate.
Epoch 135, Pointwise Training loss 0.9903175898136631, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.79938113689423, Full test loss: 11.114154040813446
The current learning rate is: 2.44140625e-06
Epoch 136 started ======>
train batch for epoch #  136 ==============>
test batch for epoch #  136 ======================>
Scheduler is currently registering the learning rate.
Epoch 136, Pointwise Training loss 0.9903249625236757, Pointwise Validation loss 0.9261795034011205
Full training loss: 122.80029535293579, Full test loss: 11.114154040813446
The current learning rate is: 2.44140625e-06
Epoch 137 started ======>
train batch for epoch #  137 ==============>
