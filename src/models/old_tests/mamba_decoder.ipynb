{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/DATA/Personnel/Other learning/Programming/Professional_Opportunities/KITE - Video Pose ViT/KITE - Video Pose Landmark Detection/mamba_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/DATA/Personnel/Other learning/Programming/Professional_Opportunities/KITE - Video Pose ViT/KITE - Video Pose Landmark Detection/mamba_env/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/DATA/Personnel/Other learning/Programming/Professional_Opportunities/KITE - Video Pose ViT/KITE - Video Pose Landmark Detection/mamba_env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2015-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "# remember that this is einstein operation, which is the special fancy way of reshaping.\n",
    "from einops import rearrange\n",
    "from timm.models.vision_transformer import _cfg\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple\n",
    "from timm.models.vision_transformer import _load_weights\n",
    "\n",
    "import math\n",
    "\n",
    "from mamba_ssm.modules.mamba_simple import Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
    "except ImportError:\n",
    "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mamba_ssm.ops.triton.layernorm.RMSNorm"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSNorm # successfully imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I am guessing this is for later usage, once more models have been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'your_model_path'\n",
    "_MODELS = {\n",
    "    \"videomamba_t16_in1k\": os.path.join(MODEL_PATH, \"videomamba_t16_in1k_res224.pth\"),\n",
    "    \"videomamba_s16_in1k\": os.path.join(MODEL_PATH, \"videomamba_s16_in1k_res224.pth\"),\n",
    "    \"videomamba_m16_in1k\": os.path.join(MODEL_PATH, \"videomamba_m16_in1k_res224.pth\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This might be the source of my confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember what module stands for https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False,drop_path=0.,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n",
    "\n",
    "        This Block has a slightly different structure compared to a regular\n",
    "        prenorm Transformer block.\n",
    "        The standard block is: LN -> MHA/MLP -> Add.\n",
    "        [Ref: https://arxiv.org/abs/2002.04745]\n",
    "        Here we have: Add -> LN -> Mixer, returning both\n",
    "        the hidden_states (output of the mixer) and the residual.\n",
    "        This is purely for performance reasons, as we can fuse add and LayerNorm.\n",
    "        The residual needs to be provided (except for the very first block).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        self.fused_add_norm = fused_add_norm\n",
    "        \n",
    "        # ! this is the mixer class that was created.\n",
    "        self.mixer = mixer_cls(dim)\n",
    "        self.norm = norm_cls(dim)\n",
    "\n",
    "        # ! https://stackoverflow.com/questions/69175642/droppath-in-timm-seems-like-a-dropout\n",
    "        # ! https://arxiv.org/abs/1603.09382, so they randomly drop some layers\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        if self.fused_add_norm:\n",
    "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
    "            assert isinstance(\n",
    "                self.norm, (nn.LayerNorm, RMSNorm)\n",
    "            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
    "\n",
    "    def forward(\n",
    "        # ! does the residual stand for this ? https://arxiv.org/pdf/2108.08659.pdf\n",
    "        self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None,\n",
    "        use_checkpoint=False\n",
    "    ):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: the sequence to the encoder layer (required).\n",
    "            residual: hidden_states = Mixer(LN(residual))\n",
    "        \"\"\"\n",
    "        if not self.fused_add_norm:\n",
    "            residual = (residual + self.drop_path(hidden_states)) if residual is not None else hidden_states\n",
    "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
    "            if self.residual_in_fp32:\n",
    "                residual = residual.to(torch.float32)\n",
    "        else:\n",
    "            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n",
    "            hidden_states, residual = fused_add_norm_fn(\n",
    "                hidden_states if residual is None else self.drop_path(hidden_states),\n",
    "                self.norm.weight,\n",
    "                self.norm.bias,\n",
    "                residual=residual,\n",
    "                prenorm=True,\n",
    "                residual_in_fp32=self.residual_in_fp32,\n",
    "                eps=self.norm.eps,\n",
    "            )\n",
    "        if use_checkpoint:\n",
    "            hidden_states = checkpoint.checkpoint(self.mixer, hidden_states, inference_params)\n",
    "        else:\n",
    "            hidden_states = self.mixer(hidden_states, inference_params=inference_params)\n",
    "        return hidden_states, residual\n",
    "\n",
    "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_block(\n",
    "    d_model,\n",
    "    ssm_cfg=None,\n",
    "    norm_epsilon=1e-5,\n",
    "    drop_path=0.,\n",
    "    rms_norm=True,\n",
    "    residual_in_fp32=True,\n",
    "    fused_add_norm=True,\n",
    "    layer_idx=None,\n",
    "    bimamba=True,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    "):\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    if ssm_cfg is None:\n",
    "        ssm_cfg = {}\n",
    "    # ! this partial function enables you to create a new class from someone else's class.\n",
    "    mixer_cls = partial(Mamba, layer_idx=layer_idx, bimamba=bimamba, **ssm_cfg, **factory_kwargs)\n",
    "    norm_cls = partial(nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon)\n",
    "\n",
    "    # ! and then, after creating a new mixer, they will create a block\n",
    "    block = Block(\n",
    "        d_model,\n",
    "        mixer_cls,\n",
    "        norm_cls=norm_cls,\n",
    "        drop_path=drop_path,\n",
    "        fused_add_norm=fused_add_norm,\n",
    "        residual_in_fp32=residual_in_fp32,\n",
    "    )\n",
    "    block.layer_idx = layer_idx\n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this, I am guessing, is to start the training NOT from scratch, but by using the weights of another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
    "def _init_weights(\n",
    "    module,\n",
    "    n_layer,\n",
    "    initializer_range=0.02,  # Now only used for embedding layer.\n",
    "    rescale_prenorm_residual=True,\n",
    "    n_residuals_per_layer=1,  # Change to 2 if we have MLP\n",
    "):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if module.bias is not None:\n",
    "            if not getattr(module.bias, \"_no_reinit\", False):\n",
    "                nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, std=initializer_range)\n",
    "\n",
    "    if rescale_prenorm_residual:\n",
    "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
    "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
    "        #   > the weights of residual layers at initialization by a factor of 1/âˆšN where N is the # of residual layers.\n",
    "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
    "        #\n",
    "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
    "        for name, p in module.named_parameters():\n",
    "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n",
    "                # We need to reinit p since this code could be called multiple times\n",
    "                # Having just p *= scale would repeatedly scale it down\n",
    "                nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
    "                with torch.no_grad():\n",
    "                    p /= math.sqrt(n_residuals_per_layer * n_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segm_init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following is that happens at the beginning of the mamba block, where the embeddings are created, given an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    # def __init__(self, img_size=224, patch_size=16, kernel_size=1, in_chans=3, embed_dim=768):\n",
    "    # ! change the patch_size to be 4, by default\n",
    "    def __init__(self, img_size=224, patch_size=4, kernel_size=1, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.tubelet_size = kernel_size\n",
    "\n",
    "        self.proj = nn.Conv3d(\n",
    "            in_chans, embed_dim, \n",
    "            kernel_size=(kernel_size, patch_size[0], patch_size[1]),\n",
    "            stride=(kernel_size, patch_size[0], patch_size[1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is the video mamba class that we are going to be inspired from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionMamba(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            img_size=224, \n",
    "            patch_size=16, \n",
    "            depth=24, \n",
    "            embed_dim=192, \n",
    "            channels=3, \n",
    "            num_classes=1000,\n",
    "            drop_rate=0.,\n",
    "            drop_path_rate=0.1,\n",
    "            ssm_cfg=None, \n",
    "            norm_epsilon=1e-5, \n",
    "            initializer_cfg=None,\n",
    "            fused_add_norm=True,\n",
    "            rms_norm=True, \n",
    "            residual_in_fp32=True,\n",
    "            bimamba=True,\n",
    "            # video\n",
    "            kernel_size=1, \n",
    "            num_frames=8, \n",
    "            fc_drop_rate=0., \n",
    "            device=None,\n",
    "            dtype=None,\n",
    "            # checkpoint\n",
    "            use_checkpoint=False,\n",
    "            checkpoint_num=0,\n",
    "        ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # follow MambaLMHeadModel\n",
    "        super().__init__()\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        self.fused_add_norm = fused_add_norm\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.checkpoint_num = checkpoint_num\n",
    "        print(f'Use checkpoint: {use_checkpoint}')\n",
    "        print(f'Checkpoint number: {checkpoint_num}')\n",
    "\n",
    "        # pretrain parameters\n",
    "        self.num_classes = num_classes\n",
    "        self.d_model = self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, \n",
    "            kernel_size=kernel_size,\n",
    "            in_chans=channels, embed_dim=embed_dim\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # ! This is only if we need to use heatmaps.\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, self.embed_dim))\n",
    "        self.temporal_pos_embedding = nn.Parameter(torch.zeros(1, num_frames // kernel_size, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.head_drop = nn.Dropout(fc_drop_rate) if fc_drop_rate > 0 else nn.Identity()\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        inter_dpr = [0.0] + dpr\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
    "        # mamba blocks\n",
    "\n",
    "        # ! they used their previously defined \"create_block\" to define their new layers.\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                create_block(\n",
    "                    embed_dim,\n",
    "                    ssm_cfg=ssm_cfg,\n",
    "                    norm_epsilon=norm_epsilon,\n",
    "                    rms_norm=rms_norm,\n",
    "                    residual_in_fp32=residual_in_fp32,\n",
    "                    fused_add_norm=fused_add_norm,\n",
    "                    layer_idx=i,\n",
    "                    bimamba=bimamba,\n",
    "                    drop_path=inter_dpr[i],\n",
    "                    **factory_kwargs,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # output head\n",
    "        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(embed_dim, eps=norm_epsilon, **factory_kwargs)\n",
    "\n",
    "        # original init\n",
    "        self.apply(segm_init_weights)\n",
    "        self.head.apply(segm_init_weights)\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "        # mamba init\n",
    "        self.apply(\n",
    "            partial(\n",
    "                _init_weights,\n",
    "                n_layer=depth,\n",
    "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "        return {\n",
    "            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
    "            for i, layer in enumerate(self.layers)\n",
    "        }\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"pos_embed\", \"cls_token\", \"temporal_pos_embedding\"}\n",
    "    \n",
    "    def get_num_layers(self):\n",
    "        return len(self.layers)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=\"\"):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    def forward_features(self, x, inference_params=None):\n",
    "        # ! this is where they patchify, and reshape the input.\n",
    "        x = self.patch_embed(x)\n",
    "        B, C, T, H, W = x.shape\n",
    "\n",
    "        # so this relinearizes the structure (C is 192, 284, etc.)\n",
    "        x = x.permute(0, 2, 3, 4, 1).reshape(B * T, H * W, C)\n",
    "\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        # concatenate\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # temporal pos\n",
    "        cls_tokens = x[:B, :1, :]\n",
    "        x = x[:, 1:]\n",
    "        x = rearrange(x, '(b t) n m -> (b n) t m', b=B, t=T)\n",
    "        x = x + self.temporal_pos_embedding\n",
    "        x = rearrange(x, '(b n) t m -> b (t n) m', b=B, t=T)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # ! This is only if we need to use heatmaps.!\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # mamba impl\n",
    "        residual = None\n",
    "        hidden_states = x\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            # ! This might be useful if we have to go through pretraining (i.e. if we run the model first on some other data)\n",
    "            if self.use_checkpoint and idx < self.checkpoint_num:\n",
    "                hidden_states, residual = layer(\n",
    "                    hidden_states, residual, inference_params=inference_params,\n",
    "                    use_checkpoint=True\n",
    "                )\n",
    "            # ! hidden state and residual are other parts of the state space model.s\n",
    "            else:\n",
    "                hidden_states, residual = layer(\n",
    "                    hidden_states, residual, inference_params=inference_params\n",
    "                )\n",
    "\n",
    "        if not self.fused_add_norm:\n",
    "            if residual is None:\n",
    "                residual = hidden_states\n",
    "            else:\n",
    "                residual = residual + self.drop_path(hidden_states)\n",
    "            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))\n",
    "        else:\n",
    "            # Set prenorm=False here since we don't need the residual\n",
    "            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm_f, RMSNorm) else layer_norm_fn\n",
    "            hidden_states = fused_add_norm_fn(\n",
    "                self.drop_path(hidden_states),\n",
    "                self.norm_f.weight,\n",
    "                self.norm_f.bias,\n",
    "                eps=self.norm_f.eps,\n",
    "                residual=residual,\n",
    "                prenorm=False,\n",
    "                residual_in_fp32=self.residual_in_fp32,\n",
    "            )\n",
    "\n",
    "        # return only cls token\n",
    "        return hidden_states[:, 0, :]\n",
    "\n",
    "    # !then you can run forward here, to change the features of the forward inference parameter \n",
    "    def forward(self, x, inference_params=None):\n",
    "        x = self.forward_features(x, inference_params)\n",
    "        # !then head is just a linear layer\n",
    "        x = self.head(self.head_drop(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am not sure what the following does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inflate_weight(weight_2d, time_dim, center=True):\n",
    "    print(f'Init center: {center}')\n",
    "    if center:\n",
    "        weight_3d = torch.zeros(*weight_2d.shape)\n",
    "        weight_3d = weight_3d.unsqueeze(2).repeat(1, 1, time_dim, 1, 1)\n",
    "        middle_idx = time_dim // 2\n",
    "        weight_3d[:, :, middle_idx, :, :] = weight_2d\n",
    "    else:\n",
    "        weight_3d = weight_2d.unsqueeze(2).repeat(1, 1, time_dim, 1, 1)\n",
    "        weight_3d = weight_3d / time_dim\n",
    "    return weight_3d\n",
    "\n",
    "\n",
    "def load_state_dict(model, state_dict, center=True):\n",
    "    state_dict_3d = model.state_dict()\n",
    "    for k in state_dict.keys():\n",
    "        if k in state_dict_3d.keys() and state_dict[k].shape != state_dict_3d[k].shape:\n",
    "            if len(state_dict_3d[k].shape) <= 3:\n",
    "                print(f'Ignore: {k}')\n",
    "                continue\n",
    "            print(f'Inflate: {k}, {state_dict[k].shape} => {state_dict_3d[k].shape}')\n",
    "            time_dim = state_dict_3d[k].shape[2]\n",
    "            state_dict[k] = inflate_weight(state_dict[k], time_dim, center=center)\n",
    "    \n",
    "    del state_dict['head.weight']\n",
    "    del state_dict['head.bias']\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### then this is where they define their models\n",
    "\n",
    "registers the following three model, probably so they can be reused?\n",
    "https://stackoverflow.com/questions/68463009/what-do-we-mean-by-register-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_model\n",
    "def videomamba_tiny(pretrained=False, **kwargs):\n",
    "    model = VisionMamba(\n",
    "        patch_size=16, \n",
    "        embed_dim=192, \n",
    "        depth=24, \n",
    "        rms_norm=True, \n",
    "        residual_in_fp32=True, \n",
    "        fused_add_norm=True, \n",
    "        **kwargs\n",
    "    )\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        print('load pretrained weights')\n",
    "        state_dict = torch.load(_MODELS[\"videomamba_t16_in1k\"], map_location='cpu')\n",
    "        load_state_dict(model, state_dict, center=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def videomamba_small(pretrained=False, **kwargs):\n",
    "    model = VisionMamba(\n",
    "        patch_size=16, \n",
    "        embed_dim=384, \n",
    "        depth=24, \n",
    "        rms_norm=True, \n",
    "        residual_in_fp32=True, \n",
    "        fused_add_norm=True, \n",
    "        **kwargs\n",
    "    )\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        print('load pretrained weights')\n",
    "        state_dict = torch.load(_MODELS[\"videomamba_s16_in1k\"], map_location='cpu')\n",
    "        load_state_dict(model, state_dict, center=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def videomamba_middle(pretrained=False, **kwargs):\n",
    "    model = VisionMamba(\n",
    "        patch_size=16, \n",
    "        embed_dim=576, \n",
    "        depth=32, \n",
    "        rms_norm=True, \n",
    "        residual_in_fp32=True, \n",
    "        fused_add_norm=True, \n",
    "        **kwargs\n",
    "    )\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        print('load pretrained weights')\n",
    "        state_dict = torch.load(_MODELS[\"videomamba_m16_in1k\"], map_location='cpu')\n",
    "        load_state_dict(model, state_dict, center=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from fvcore.nn import flop_count_table\n",
    "import numpy as np\n",
    "\n",
    "seed = 4217\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "num_frames = 8\n",
    "img_size = 224\n",
    "\n",
    "# # To evaluate GFLOPs, pleaset set `rms_norm=False` and `fused_add_norm=False`\n",
    "# model = videomamba_middle(num_frames=num_frames).cuda()\n",
    "# flops = FlopCountAnalysis(model, torch.rand(1, 3, num_frames, img_size, img_size).cuda())\n",
    "# s = time.time()\n",
    "# print(flop_count_table(flops, max_depth=1))\n",
    "# print(time.time()-s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
