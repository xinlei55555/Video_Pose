Read successful, here are the characteristics of your model: 
{'model_name': 'VideoMambaPose', 'project_dir': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/latent_space_regression_with_linear', 'model_type': 'latent_space_linear_regression', 'full_debug': False, 'show_gradients': False, 'show_predictions': False, 'embed_channels': 192, 'num_mamba_blocks': 12, 'num_deconv': 2, '2d_deconv': True, 'deconv_channels': 192, 'num_conv': 2, 'conv_channels': 256, 'joint_regressor': True, 'hidden_channels': 512, 'output_dimensions': 2, 'dropout': False, 'dropout_percent': 0.25, 'num_hidden_layers': 3, 'epoch_number': 300, 'batch_size': 4, 'learning_rate': 0.001, 'scheduler': True, 'start_epoch': 1, 'num_cpus': 1, 'num_gpus': 1, 'parallelize': False, 'checkpoint_directory': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/src/models/experiments/heatmap/checkpoint', 'checkpoint_name': 'cropped_tanh_overfit_model', 'follow_up': False, 'previous_training_epoch': 1, 'previous_checkpoint': '', 'dataset_name': 'JHMDB', 'data_path': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/data/JHMDB', 'annotations_path': '/home/xinlei/Projects/KITE_MambaPose/Video_Pose/3_VideoMambaPose/data/JHMDB_old/annotations', 'use_videos': True, 'preprocess_videos': True, 'skip': ['wave'], 'num_frames': 16, 'channels': 3, 'image_height': 240, 'image_width': 320, 'image_tensor_height': 256, 'image_tensor_width': 192, 'patch_number': 192, 'patch_size': 16, 'jump': 1, 'joint_number': 15, 'real_job': False, 'normalized': True, 'default': False, 'min_norm': -1}
[2024-06-10 23:55:34,912] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
torch.Size([40, 240, 320, 3])
The passed width and height are  320 240
Video saved as results/4normalized_pull_ups
Use checkpoint: False
Checkpoint number: 0
HeatMapVideoMambaPose(
  (mamba): VisionMamba(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 192, kernel_size=(1, 16, 16), stride=(1, 16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (head_drop): Identity()
    (head): Linear(in_features=192, out_features=1000, bias=True)
    (drop_path): DropPath()
    (layers): ModuleList(
      (0-1): 2 x Block(
        (mixer): Mamba(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (act): SiLU()
          (x_proj): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj): Linear(in_features=12, out_features=384, bias=True)
          (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
        (norm): RMSNorm()
        (drop_path): Identity()
      )
      (2-11): 10 x Block(
        (mixer): Mamba(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (act): SiLU()
          (x_proj): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj): Linear(in_features=12, out_features=384, bias=True)
          (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
          (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
        (norm): RMSNorm()
        (drop_path): DropPath()
      )
    )
    (norm_f): RMSNorm()
  )
  (deconv): Deconv(
    (conv_layers): Sequential(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))
    )
    (deconv_layers): Sequential(
      (0): ConvTranspose2d(192, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (joints): JointOutput(
    (regressor): Sequential(
      (0): Linear(in_features=3072, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=2, bias=True)
      (3): Tanh()
    )
  )
)
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([1, 15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
Are the last two outputs the same?:  tensor([[False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False],
        [False, False]])
output tensor([[93.7185, 38.5342],
        [93.6168, 39.1018],
        [93.6390, 38.5031],
        [93.0055, 39.6142],
        [93.2653, 38.7107],
        [94.0881, 39.0666],
        [93.5197, 39.3733],
        [94.3561, 38.2990],
        [93.0889, 38.8016],
        [94.4323, 39.5472],
        [93.5427, 39.6913],
        [93.7659, 38.2886],
        [92.6726, 38.7655],
        [92.5344, 40.2696],
        [92.5344, 40.2696]])
The passed width and height are  320 240
Video saved as results/2predicted
